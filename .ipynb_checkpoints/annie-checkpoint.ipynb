{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfad1e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import time\n",
    "import json\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.spatial.distance import cdist, cosine, euclidean\n",
    "from scipy.stats import ttest_ind, ttest_1samp\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from gpu_utils import restrict_GPU_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1b5ec5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU:2\n"
     ]
    }
   ],
   "source": [
    "restrict_GPU_pytorch('2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ace3eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list of predictions (prob of moderation) for a list of examples and a given model. \n",
    "# allExamples: list of strings \n",
    "# model: fastText model object\n",
    "def getModelPredsHelper(allExamples, model):\n",
    "    exampleList = [x.replace('\\n', ' ') for x in allExamples]\n",
    "    preds = model.predict(exampleList)\n",
    "    preds_int = np.array([1 if 'positive' in p[0] else 0 for p in preds[0]])\n",
    "    preds_prob = np.array([p[0] for p in preds[1]])\n",
    "    probs = np.array([1 - p if preds_int[i] == 0 else p for (i,p) in enumerate(preds_prob)])\n",
    "    probs = [np.round(p,3) for p in probs]\n",
    "    return probs\n",
    "\n",
    "# vec: query vector (list of floats)\n",
    "# all_vecs: pool of all vectors from which neighbors are retrieved (list of list of floats)\n",
    "# comments: comments corresponding to the vectors in all_vecs (list of strings)\n",
    "# n: number of neighbors to return (int)\n",
    "# return_idx: if true, returns indices of nearest neighbors instead of the actual comments (bool)\n",
    "def getKNNFromVector(vec, all_vecs, comments, n=30, return_idx=False):\n",
    "    dist_vec = cdist(vec, all_vecs, 'cosine')\n",
    "    top_vec_idx = np.argsort(dist_vec[0])[1:n+1]\n",
    "    if return_idx: return top_vec_idx\n",
    "    top_comments = np.array(comments)[top_vec_idx]\n",
    "    top_comments = [c for c in top_comments]\n",
    "    return top_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27eb67f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FPATH = '../data/reddit/'\n",
    "MODEL_FPATH = '../models/reddit/'\n",
    "USE_PATH = '../Dev/tf_hub/universal-sentence-encoder_4/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a9c46e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = ['askscience', 'conspiracy', 'funny', 'hillaryclinton', 'history']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88050177",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a995422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>moderated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>You can tell this is fake because it shows a b...</td>\n",
       "      <td>funny</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>They're purple because she's dead.\\n\\nLol sorr...</td>\n",
       "      <td>hillaryclinton</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The fat lady is singing.</td>\n",
       "      <td>funny</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Good thing volks never break down. XD Shitty k...</td>\n",
       "      <td>funny</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>I'm dreaming of a bright christmas.\\n</td>\n",
       "      <td>funny</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               body  \\\n",
       "0           0  You can tell this is fake because it shows a b...   \n",
       "1           1  They're purple because she's dead.\\n\\nLol sorr...   \n",
       "2           2                           The fat lady is singing.   \n",
       "3           3  Good thing volks never break down. XD Shitty k...   \n",
       "4           5              I'm dreaming of a bright christmas.\\n   \n",
       "\n",
       "        subreddit  moderated  \n",
       "0           funny          1  \n",
       "1  hillaryclinton          1  \n",
       "2           funny          1  \n",
       "3           funny          1  \n",
       "4           funny          1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_comments = pd.read_csv(DATA_FPATH + 'all_comments_df')\n",
    "embed = hub.load(USE_PATH);\n",
    "all_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36b7c9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "sub = 'funny'\n",
    "comments = all_comments[all_comments.subreddit == sub].body.values\n",
    "labels = all_comments[all_comments.subreddit == sub].moderated.values\n",
    "vecs = np.array(embed(comments))\n",
    "subreddit_model = fasttext.load_model(MODEL_FPATH + \"%s_model.bin\" % sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f92ee1a",
   "metadata": {},
   "source": [
    "## Nearest Neighbor Search for Example Set Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7949c4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = \"\"\"\n",
    "poor snowflake do you need a safe space\n",
    "\"\"\"\n",
    "\n",
    "top_vec_idx = getKNNFromVector(embed([seed]), vecs, comments, n=50, return_idx=True)\n",
    "knn_vecs = vecs[top_vec_idx]\n",
    "knn_comments = comments[top_vec_idx]\n",
    "num_clusters = 4\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(knn_vecs)\n",
    "\n",
    "for cluster in range(num_clusters):\n",
    "    cluster_comments = knn_comments[np.where(kmeans.labels_ == cluster)]\n",
    "    \n",
    "    print('===================== CLUSTER %i =========================' % cluster)\n",
    "    for c in cluster_comments:\n",
    "        print(c, end='\\n------------------------\\n')\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305d6727",
   "metadata": {},
   "source": [
    "### Next steps: Annie\n",
    "\n",
    "Example search & generation has three main steps (though they may be intertwined in some ways): 1) getting similar examples, 2) clustering or organizing them in some way 3) visualizing the result.  Right now we're doing 1) with euclidean distance in embedding space of the USE, 2) with k-means clustering, and 3) with just printing out the examples. \n",
    "\n",
    "We can think about ways to improve each of these parts.  To start, let's explore 1).  Right now we are getting similar examples with the USE (the model we load from TF Hub).  How does this compare to a different embedding model?  Here are some we can try: \n",
    "* BERT: https://huggingface.co/bert-base-uncased\n",
    "* RoBERTa: https://huggingface.co/roberta-base\n",
    "* XLNet: https://huggingface.co/xlnet-base-cased\n",
    "\n",
    "Later, we could also think about fine-tuning some of these embeddings to be better suited to our task/data.  Sample code for loading BERT and getting embeddings for a sample of sentences: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b2d08d",
   "metadata": {},
   "source": [
    "### To do: \n",
    "* modify `getKNNFromVector` to take a particular embedding name (e.g., 'BERT') and compute distances in that embedding space. \n",
    "* for a handful of seed sentences, get the nearest neighbors and print them out as above in each of the different embedding spaces. Qualitatively note differences you notice among what is returned as similar. Do some seem better or worse?  Are there noticeable differences? \n",
    "* for an easy 2 or 3D projection, you can try loading the data into the embedding projector: https://projector.tensorflow.org/  You may want to just do a particular example and its 100 nearest neighbors or something (rather than all the data). You'll have to save the embeddings and sentences as TSV files and then load them in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6d6f824",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (1): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (2): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (3): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (4): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (5): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "from transformers import DistilBertTokenizerFast, DistilBertModel\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# bert_model.cuda()\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "model.eval()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d93e1461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93c03f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03d2c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sentences = [\"Hello, my dog is cute\", \"another sample sentence\"]\n",
    "tokenized_sentences = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"pt\")['input_ids']\n",
    "tokenized_sentences = tokenized_sentences.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0289197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750cdb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(tokenized_sentences)['last_hidden_state'].cpu().detach().numpy()\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dce2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CommentsDataset(Dataset):\n",
    "    def __init__(self, comments, labels, tokenizerfn):\n",
    "        self.comments = comments\n",
    "        self.labels = labels\n",
    "        self.tokenizerfn = lambda comments: tokenizerfn(comments, padding=True, truncation=True, return_tensors=\"pt\")['input_ids']\n",
    "#         self.tokenizer = tokenizerfn\n",
    "#         self.tokenizerfn = lambda comments: self.tokenizer(comments, padding=True, truncation=True, return_tensors=\"pt\")['input_ids']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comments)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokenizerfn(comments[idx]) #, self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ff80fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer # , BertModel\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "dataset = CommentsDataset(comments, labels, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30166441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(dataloader))\n",
    "dataset[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df95fafa",
   "metadata": {},
   "source": [
    "#### BERT embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e7b552b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_obj():\n",
    "    import gc\n",
    "    GPU_count = 0\n",
    "    Pinned_count = 0\n",
    "    for tracked_object in gc.get_objects():\n",
    "        if torch.is_tensor(tracked_object):\n",
    "            if tracked_object.is_cuda:\n",
    "                GPU_count+=1\n",
    "            if tracked_object.is_pinned():\n",
    "                Pinned_count+=1\n",
    "            \n",
    "\n",
    "    print(\"There are {} cuda objects\".format(GPU_count))\n",
    "    print(\"There are {} pinned objects\".format(Pinned_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2295b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 34347\n",
      "There are 101 cuda objects\n",
      "There are 0 pinned objects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/annieb22/.local/lib/python3.6/site-packages/torch-1.3.1-py3.6-linux-x86_64.egg/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 of 34347\n",
      "There are 101 cuda objects\n",
      "There are 0 pinned objects\n",
      "200 of 34347\n",
      "There are 101 cuda objects\n",
      "There are 0 pinned objects\n",
      "300 of 34347\n",
      "There are 101 cuda objects\n",
      "There are 0 pinned objects\n",
      "400 of 34347\n",
      "There are 101 cuda objects\n",
      "There are 0 pinned objects\n",
      "500 of 34347\n",
      "There are 101 cuda objects\n",
      "There are 0 pinned objects\n",
      "600 of 34347\n",
      "There are 101 cuda objects\n",
      "There are 0 pinned objects\n",
      "700 of 34347\n",
      "There are 101 cuda objects\n",
      "There are 0 pinned objects\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 266.00 MiB (GPU 0; 11.91 GiB total capacity; 10.19 GiB already allocated; 117.94 MiB free; 317.25 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6b3743c6d0c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtokenized_comments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_comments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'last_hidden_state'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#.cpu().detach().numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mtokenized_comments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch-1.3.1-py3.6-linux-x86_64.egg/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tta-project/lib/python3.6/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         )\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch-1.3.1-py3.6-linux-x86_64.egg/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tta-project/lib/python3.6/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             layer_outputs = layer_module(\n\u001b[0;32m--> 328\u001b[0;31m                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m             )\n\u001b[1;32m    330\u001b[0m             \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch-1.3.1-py3.6-linux-x86_64.egg/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tta-project/lib/python3.6/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# Feed Forward Network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0mffn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m         \u001b[0mffn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mffn_output\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msa_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch-1.3.1-py3.6-linux-x86_64.egg/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tta-project/lib/python3.6/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mapply_chunking_to_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mff_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tta-project/lib/python3.6/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   2347\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2349\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/tta-project/lib/python3.6/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mff_chunk\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mff_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tta-project/lib/python3.6/site-packages/transformers/activations.py\u001b[0m in \u001b[0;36mgelu_python\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mAlso\u001b[0m \u001b[0msee\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mGaussian\u001b[0m \u001b[0mError\u001b[0m \u001b[0mLinear\u001b[0m \u001b[0mUnits\u001b[0m \u001b[0mpaper\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0marxiv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1606.08415\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \"\"\"\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 266.00 MiB (GPU 0; 11.91 GiB total capacity; 10.19 GiB already allocated; 117.94 MiB free; 317.25 MiB cached)"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "for i in range(len(comments)):\n",
    "    tokenized_comments = tokenizer(list(comments[i]), padding=True, truncation=True, return_tensors=\"pt\")['input_ids'].cuda()\n",
    "    output = model(tokenized_comments)['last_hidden_state'] #.cpu().detach().numpy()\n",
    "    del tokenized_comments\n",
    "    if i % 100 == 0:\n",
    "        print(f'{i} of 34347')\n",
    "        print_gpu_obj()\n",
    "t1 = time.time()\n",
    "print(f'{t1 - t0} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416e5bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_comments = tokenizer(list(comments), padding=True, truncation=True, return_tensors=\"pt\")['input_ids']\n",
    "tokenized_comments = tokenized_comments.cuda()\n",
    "tokenized_comments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1b4095",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_output = model(tokenized_comments)['last_hidden_state'].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3bbff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd41a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "for i in range(len(comments)):\n",
    "    tokenized_comment = tokenizer(list(comments[i]), padding=True, truncation=True, return_tensors=\"pt\")['input_ids'].cuda()\n",
    "    output = bert_model(tokenized_comment)['pooler_output'].detach()\n",
    "    if i % 1000 == 0:\n",
    "        print(f'{i} of 34347')\n",
    "t1 = time.time()\n",
    "print(f'{t1 - t0} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee17ad94",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_output = bert_model(tokenized_comment)['pooler_output'].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375b8ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97534e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 1000\n",
    "size = 34347//num_iterations\n",
    "for i in range(num_iterations):\n",
    "    tokenized_comments = tokenizer(list(comments[size*i : min(size*(i+1), 34347)]), padding=True, truncation=True, return_tensors=\"pt\")['input_ids'].cuda()\n",
    "    bert_output = bert_model(tokenized_comments)['pooler_output'].detach().shape()\n",
    "    print(bert_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e780708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comments_bert_model(comments, batch_size):\n",
    "    comments_output = None\n",
    "    num_iterations = 1+len(comments)//batch_size\n",
    "    for i in range(num_iterations):\n",
    "        small_sample_comments = list(comments[batch_size*i : min(batch_size*(i+1), len(comments))])\n",
    "        tokenized_small_sample_comments = tokenizer(small_sample_comments, padding=True, truncation=True, return_tensors=\"pt\")['input_ids']\n",
    "        tokenized_small_sample_comments.cuda()\n",
    "        small_sample_comments_output = model(tokenized_small_sample_comments)['pooler_output'].cpu().detach().numpy()\n",
    "        print(f'{i+1}/{num_iterations} iterations complete')\n",
    "        if comments_output is None:\n",
    "            comments_output = small_sample_comments_output\n",
    "        else:\n",
    "            comments_output = np.concatenate((comments_output, small_sample_comments_output), axis=0)\n",
    "    return comments_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbe0031",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_comments_output = comments_bert_model(comments, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d44b19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc74468",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_comments = tokenizer(list(comments), padding=True, truncation=True, return_tensors=\"pt\")['input_ids']\n",
    "output = bert_model(tokenized_comments) #['pooler_output'].detach().numpy()\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aac277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comments_bert_model_universal_sentence_encoder(comments, batch_size):\n",
    "    comments_output = None\n",
    "    num_iterations = 1+len(comments)//batch_size\n",
    "    for i in range(num_iterations):\n",
    "        small_sample_comments = list(comments[batch_size*i : min(batch_size*(i+1), len(comments))])\n",
    "        tokenized_small_sample_comments = torch.from_numpy(embed(small_sample_comments).numpy()).long()\n",
    "        small_sample_comments_output = bert_model(tokenized_small_sample_comments)['pooler_output'].detach().numpy()\n",
    "        print(f'{i+1}/{num_iterations} iterations complete')\n",
    "        if comments_output is None:\n",
    "            comments_output = small_sample_comments_output\n",
    "        else:\n",
    "            comments_output = np.concatenate((comments_output, small_sample_comments_output), axis=0)\n",
    "    return comments_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6257d3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "universal_comments_output = comments_bert_model_universal_sentence_encoder(comments, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c7a2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = {'BERT': bert_comments_output, 'universal-sentence-encoder_4': embed(comments)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7220ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeds: query comments (list of strings)\n",
    "# comments: comments to compare to (list of strings)\n",
    "# embedding: string corresponding to a particular embedding name (e.g. 'BERT')\n",
    "# n: number of neighbors to return (int)\n",
    "# return_idx: if true, returns indices of nearest neighbors instead of the actual comments (bool)\n",
    "def getKNNFromVector(seeds, comments, embedding, n=30, return_idx=False):\n",
    "    comments_output = outputs[embedding]\n",
    "    if embedding == 'BERT':\n",
    "        tokenized_seeds = tokenizer(seeds, padding=True, truncation=True, return_tensors=\"pt\")['input_ids']\n",
    "    elif embedding == 'universal-sentence-encoder_4':\n",
    "        tokenized_seeds = embed(seeds)\n",
    "    else:\n",
    "        return \"invalid embedding provided\"\n",
    "    \n",
    "    seeds_output = bert_model(tokenized_seeds)['pooler_output'].detach().numpy()\n",
    "    dist_vec = cdist(seeds_output, comments_output, 'cosine')\n",
    "    top_vec_idx = np.argsort(dist_vec[0])\n",
    "    top_comments = np.array(comments[:batch_size*num_iterations])[top_vec_idx]\n",
    "    top_n_comments = [c for c in top_comments[:n]]\n",
    "    return top_n_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cfb3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_embeddings(seeds, comments):\n",
    "    if not isinstance(seeds, list):\n",
    "        print(\"Must input a list of seeds.\")\n",
    "        return\n",
    "    else:\n",
    "        if len(seeds) == 1:\n",
    "            print(f'Testing on the seed \"{decode(seeds[0])}\"')\n",
    "        else:\n",
    "            s = \", \".join([f'\"{decode(seed)}\"' for seed in seeds])\n",
    "            print(f'Testing on the seeds {s}')\n",
    "    \n",
    "    print('\\nResults with universal_sentence_encoder_4:')\n",
    "    try:\n",
    "        embed_results = getKNNFromVector(seeds, comments, n=10, return_idx=False)\n",
    "        for (i, x) in enumerate(embed_results):\n",
    "            print(f'{i+1}. \"{decode(x)}\"')\n",
    "    except:\n",
    "        print(\"something failed\")\n",
    "    \n",
    "    print('\\nResults with BERT:')\n",
    "    try:\n",
    "        bert_results = getKNNFromVector(seeds, comments, embedding=\"BERT\", n=10, return_idx=False)\n",
    "        for (i,x) in enumerate(bert_results):\n",
    "            print(f'{i+1}. \"{decode(x)}\"')\n",
    "    except:\n",
    "        print(\"something failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99dc065",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [\"hello\"]\n",
    "getKNNFromVector(seeds, list(comments), embedding=\"BERT\", n=10, return_idx=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f6f5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [\"hello\"]\n",
    "compare_embeddings(seeds, comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74914459",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [\"\\npoor snowflake do you need a safe space\\n\"]\n",
    "compare_embeddings(seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbf00f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [\"Hello, my dog is cute\", \"another sample sentence\"]\n",
    "compare_embeddings(seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fcb2f1",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f84525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantify_shift(shift_target, example_list, example_list_2, model, labels = None, labels_2 = None, list_names=[\"list 1\", \"list 2\"]): \n",
    "    if shift_target == \"predictions\":\n",
    "        preds_1 = getModelPredsHelper(example_list, model)\n",
    "        preds_2 = getModelPredsHelper(example_list_2, model)\n",
    "        ttest_result = ttest_ind(preds_1, preds_2)\n",
    "        if ttest_result.pvalue >= 0.05: \n",
    "            print(\"Predictions are not significantly different.\")\n",
    "        else: \n",
    "            operator = \"higher\" if ttest_result.statistic > 0 else \"lower\"\n",
    "            print(\"P(moderated) for %s is *%s* than for %s (pval = %.3f)\" % (list_names[0], operator, list_names[1], ttest_result.pvalue))\n",
    "            \n",
    "            \n",
    "    elif shift_target == \"representation\":\n",
    "        vecs_1 = [model.get_sentence_vector(ex) for ex in example_list]\n",
    "        vecs_2 = [model.get_sentence_vector(ex) for ex in example_list_2]\n",
    "        intergroup_diffs = cdist(vecs_1, vecs_2).flatten()\n",
    "        intragroup_diffs = np.concatenate((cdist(vecs_1, vecs_1).flatten(), cdist(vecs_2, vecs_2).flatten()))\n",
    "        ttest_result = ttest_ind(intergroup_diffs, intragroup_diffs)\n",
    "        if ttest_result.statistic > 0 and ttest_result.pvalue < 0.05: \n",
    "            print(\"Representations for %s are significantly different from %s (pval = %f).\" % (list_names[0], list_names[1], ttest_result.pvalue))\n",
    "        else: \n",
    "            print(\"Representations for %s and %s are not significantly different.\" % (list_names[0], list_names[1]))\n",
    "            \n",
    "            \n",
    "    elif shift_target == \"performance\":\n",
    "        preds_1 = getModelPredsHelper(example_list, model)\n",
    "        preds_2 = getModelPredsHelper(example_list_2, model)\n",
    "        perf_1 = np.array([np.round(preds_1[i]) == labels[i] for i in range(len(preds_1))]).astype(int)\n",
    "        perf_2 = np.array([np.round(preds_2[i]) == labels_2[i] for i in range(len(preds_2))]).astype(int)\n",
    "        ttest_result = ttest_ind(perf_1, perf_2)\n",
    "        if ttest_result.pvalue >= 0.05: \n",
    "            print(\"Model performance on %s and %s is not significantly different.\" % (list_names[0], list_names[1]))\n",
    "        else: \n",
    "            operator = \"higher\" if ttest_result.statistic > 0 else \"lower\"\n",
    "            print(\"Model performance on %s is *%s* than for %s (pval = %.3f)\" % (list_names[0], operator, list_names[1], ttest_result.pvalue))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
