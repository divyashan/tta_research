{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Using GPU:3\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Set-up: Import numpy and assign GPU\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/local/helenl/.cache/'\n",
    "os.environ['PYTORCH_TRANSFORMERS_CACHE'] = '/local/helenl/.cache/'\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from gpu_utils import restrict_GPU_pytorch\n",
    "from helenl_utils import *\n",
    "from matplot_figure_format import *\n",
    "from transforms_helenl import *\n",
    "from path_stop import *\n",
    "\n",
    "restrict_GPU_pytorch('3')\n",
    "\n",
    "import statistics\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import json\n",
    "import random\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import argparse\n",
    "import pdb\n",
    "\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corrections_corruptions_plots(df, title = ''):\n",
    "    \n",
    "    sns.set(font_scale = 2)\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    df_melted = df.reset_index().melt(id_vars = 'augmentation')\n",
    "    df_melted = df_melted[df_melted.variable != 'index']\n",
    "\n",
    "    print(df_melted)\n",
    "    \n",
    "    min_value = df_melted.min()[\"value\"]\n",
    "    max_value = df_melted.max()[\"value\"]\n",
    "\n",
    "    ax.set_ylim(min_value - 100, max_value + 100)\n",
    "\n",
    "\n",
    "    #show_values(p)\n",
    "    plt.rcParams['figure.figsize'] = (80,20)\n",
    "\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "    plt.title(title)\n",
    "\n",
    "    \n",
    "\n",
    "    #sns.set(rc = {'figure.figsize':(30,10)})\n",
    "    p = sns.barplot(x = 'augmentation', y = 'value', hue = 'variable', data = df_melted, ci = False, palette = [\"green\", \"red\"])#, order = acc_df['augmentation'])\n",
    "\n",
    "    show_values(p)\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_augmented_labels() num_samples: 4\n",
      "initialize_transform() num_samples: 4\n",
      "char_min: 1\n",
      "word_p: 0.1\n",
      "initialize_bert_transform() num_samples: 4\n",
      "initialize_transform() num_samples: 4\n",
      "char_min: 1\n",
      "word_p: 0.1\n",
      "initialize_bert_transform() num_samples: 4\n",
      "nlp_random_word_delete\n",
      "initialize model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertClassifier: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertClassifier were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialization complete\n",
      "generating predictions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5871f2e1a044402781c8b519bd8f31d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16723 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: [[tensor([[[  101,     1],\n",
      "         [ 2821,     1],\n",
      "         [ 2748,     1],\n",
      "         ...,\n",
      "         [    0,     0],\n",
      "         [    0,     0],\n",
      "         [    0,     0]],\n",
      "\n",
      "        [[  101,     1],\n",
      "         [ 2002,     1],\n",
      "         [ 1005,     1],\n",
      "         ...,\n",
      "         [    0,     0],\n",
      "         [    0,     0],\n",
      "         [    0,     0]],\n",
      "\n",
      "        [[  101,     1],\n",
      "         [ 3393,     1],\n",
      "         [ 2721,     1],\n",
      "         ...,\n",
      "         [    0,     0],\n",
      "         [    0,     0],\n",
      "         [    0,     0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  101,     1],\n",
      "         [ 9951,     1],\n",
      "         [ 1010,     1],\n",
      "         ...,\n",
      "         [    0,     0],\n",
      "         [    0,     0],\n",
      "         [    0,     0]],\n",
      "\n",
      "        [[  101,     1],\n",
      "         [ 1998,     1],\n",
      "         [ 2009,     1],\n",
      "         ...,\n",
      "         [    0,     0],\n",
      "         [    0,     0],\n",
      "         [    0,     0]],\n",
      "\n",
      "        [[  101,     1],\n",
      "         [ 1996,     1],\n",
      "         [21003,     1],\n",
      "         ...,\n",
      "         [    0,     0],\n",
      "         [    0,     0],\n",
      "         [    0,     0]]]), tensor([[[ 101,    1],\n",
      "         [2821,    1],\n",
      "         [2748,    1],\n",
      "         ...,\n",
      "         [   0,    0],\n",
      "         [   0,    0],\n",
      "         [   0,    0]],\n",
      "\n",
      "        [[ 101,    1],\n",
      "         [2002,    1],\n",
      "         [1005,    1],\n",
      "         ...,\n",
      "         [   0,    0],\n",
      "         [   0,    0],\n",
      "         [   0,    0]],\n",
      "\n",
      "        [[ 101,    1],\n",
      "         [3393,    1],\n",
      "         [2721,    1],\n",
      "         ...,\n",
      "         [   0,    0],\n",
      "         [   0,    0],\n",
      "         [   0,    0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 101,    1],\n",
      "         [9951,    1],\n",
      "         [1010,    1],\n",
      "         ...,\n",
      "         [   0,    0],\n",
      "         [   0,    0],\n",
      "         [   0,    0]],\n",
      "\n",
      "        [[ 101,    1],\n",
      "         [1998,    1],\n",
      "         [2009,    1],\n",
      "         ...,\n",
      "         [   0,    0],\n",
      "         [   0,    0],\n",
      "         [   0,    0]],\n",
      "\n",
      "        [[ 101,    1],\n",
      "         [1996,    1],\n",
      "         [3124,    1],\n",
      "         ...,\n",
      "         [   0,    0],\n",
      "         [   0,    0],\n",
      "         [   0,    0]]]), tensor([[[ 101,    1],\n",
      "         [2821,    1],\n",
      "         [2748,    1],\n",
      "         ...,\n",
      "         [   0,    0],\n",
      "         [   0,    0],\n",
      "         [   0,    0]],\n",
      "\n",
      "        [[ 101,    1],\n",
      "         [2002,    1],\n",
      "         [1005,    1],\n",
      "         ...,\n",
      "         [   0,    0],\n",
      "         [   0,    0],\n",
      "         [   0,    0]],\n",
      "\n",
      "        [[ 101,    1],\n",
      "         [1010,    1],\n",
      "         [2017,    1],\n",
      "         ...,\n",
      "         [   0,    0],\n",
      "         [   0,    0],\n",
      "         [   0,    0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 101,    1],\n",
      "         [9951,    1],\n",
      "         [1010,    1],\n",
      "         ...,\n",
      "         [   0,    0],\n",
      "         [   0,    0],\n",
      "         [   0,    0]],\n",
      "\n",
      "        [[ 101,    1],\n",
      "         [1998,    1],\n",
      "         [2045,    1],\n",
      "         ...,\n",
      "         [   0,    0],\n",
      "         [   0,    0],\n",
      "         [   0,    0]],\n",
      "\n",
      "        [[ 101,    1],\n",
      "         [1996,    1],\n",
      "         [3124,    1],\n",
      "         ...,\n",
      "         [   0,    0],\n",
      "         [   0,    0],\n",
      "         [   0,    0]]]), tensor([[[ 101,    1],\n",
      "         [2821,    1],\n",
      "         [2748,    1],\n",
      "         ...,\n",
      "         [   0,    0],\n",
      "         [   0,    0],\n",
      "         [   0,    0]],\n",
      "\n",
      "        [[ 101,    1],\n",
      "         [2002,    1],\n",
      "         [1005,    1],\n",
      "         ...,\n",
      "         [   0,    0],\n",
      "         [   0,    0],\n",
      "         [   0,    0]],\n",
      "\n",
      "        [[ 101,    1],\n",
      "         [3393,    1],\n",
      "         [2721,    1],\n",
      "         ...,\n",
      "         [   0,    0],\n",
      "         [   0,    0],\n",
      "         [   0,    0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 101,    1],\n",
      "         [9951,    1],\n",
      "         [1010,    1],\n",
      "         ...,\n",
      "         [   0,    0],\n",
      "         [   0,    0],\n",
      "         [   0,    0]],\n",
      "\n",
      "        [[ 101,    1],\n",
      "         [1998,    1],\n",
      "         [2045,    1],\n",
      "         ...,\n",
      "         [   0,    0],\n",
      "         [   0,    0],\n",
      "         [   0,    0]],\n",
      "\n",
      "        [[ 101,    1],\n",
      "         [1996,    1],\n",
      "         [3124,    1],\n",
      "         ...,\n",
      "         [   0,    0],\n",
      "         [   0,    0],\n",
      "         [   0,    0]]])], tensor([1, 0, 0, 1, 0, 1, 1, 0]), tensor([[0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])]\n",
      "batch length: 3\n",
      "batch[0] length: 4\n",
      "sample_size: torch.Size([8, 300, 2])\n",
      "raw pred length: 8\n",
      "pre-predictions: []\n",
      "post-predictions: [[-0.44154849648475647, 0.34076300263404846], [2.9877212047576904, -3.2341203689575195], [2.0545804500579834, -2.339855432510376], [-1.3853970766067505, 1.3434847593307495], [2.835056781768799, -3.1024301052093506], [-1.9354890584945679, 1.8553491830825806], [-0.5917312502861023, 0.49990591406822205], [2.665104389190674, -2.9481394290924072]]\n",
      "batch[0] length: 4\n",
      "sample_size: torch.Size([8, 300, 2])\n",
      "raw pred length: 8\n",
      "pre-predictions: [[-0.44154849648475647, 0.34076300263404846], [2.9877212047576904, -3.2341203689575195], [2.0545804500579834, -2.339855432510376], [-1.3853970766067505, 1.3434847593307495], [2.835056781768799, -3.1024301052093506], [-1.9354890584945679, 1.8553491830825806], [-0.5917312502861023, 0.49990591406822205], [2.665104389190674, -2.9481394290924072]]\n",
      "post-predictions: [[-0.44154849648475647, 0.34076300263404846], [2.9877212047576904, -3.2341203689575195], [2.0545804500579834, -2.339855432510376], [-1.3853970766067505, 1.3434847593307495], [2.835056781768799, -3.1024301052093506], [-1.9354890584945679, 1.8553491830825806], [-0.5917312502861023, 0.49990591406822205], [2.665104389190674, -2.9481394290924072], [-0.8192386031150818, 0.7110495567321777], [2.3866820335388184, -2.6294047832489014], [1.8687490224838257, -2.1366488933563232], [-1.9705138206481934, 1.8934345245361328], [3.027294397354126, -3.2236993312835693], [-2.014310121536255, 1.9314030408859253], [-0.5917312502861023, 0.49990591406822205], [1.9123210906982422, -2.150873899459839]]\n",
      "batch[0] length: 4\n",
      "sample_size: torch.Size([8, 300, 2])\n",
      "raw pred length: 8\n",
      "pre-predictions: [[-0.44154849648475647, 0.34076300263404846], [2.9877212047576904, -3.2341203689575195], [2.0545804500579834, -2.339855432510376], [-1.3853970766067505, 1.3434847593307495], [2.835056781768799, -3.1024301052093506], [-1.9354890584945679, 1.8553491830825806], [-0.5917312502861023, 0.49990591406822205], [2.665104389190674, -2.9481394290924072], [-0.8192386031150818, 0.7110495567321777], [2.3866820335388184, -2.6294047832489014], [1.8687490224838257, -2.1366488933563232], [-1.9705138206481934, 1.8934345245361328], [3.027294397354126, -3.2236993312835693], [-2.014310121536255, 1.9314030408859253], [-0.5917312502861023, 0.49990591406822205], [1.9123210906982422, -2.150873899459839]]\n",
      "post-predictions: [[-0.44154849648475647, 0.34076300263404846], [2.9877212047576904, -3.2341203689575195], [2.0545804500579834, -2.339855432510376], [-1.3853970766067505, 1.3434847593307495], [2.835056781768799, -3.1024301052093506], [-1.9354890584945679, 1.8553491830825806], [-0.5917312502861023, 0.49990591406822205], [2.665104389190674, -2.9481394290924072], [-0.8192386031150818, 0.7110495567321777], [2.3866820335388184, -2.6294047832489014], [1.8687490224838257, -2.1366488933563232], [-1.9705138206481934, 1.8934345245361328], [3.027294397354126, -3.2236993312835693], [-2.014310121536255, 1.9314030408859253], [-0.5917312502861023, 0.49990591406822205], [1.9123210906982422, -2.150873899459839], [-0.06651907414197922, -0.01615358702838421], [2.47937273979187, -2.7413527965545654], [2.025391101837158, -2.337613105773926], [-2.0436043739318848, 1.9475136995315552], [2.8817620277404785, -3.109703540802002], [-1.8063997030258179, 1.734178066253662], [-0.3967443108558655, 0.31634753942489624], [2.0342674255371094, -2.2393856048583984]]\n",
      "batch[0] length: 4\n",
      "sample_size: torch.Size([8, 300, 2])\n",
      "raw pred length: 8\n",
      "pre-predictions: [[-0.44154849648475647, 0.34076300263404846], [2.9877212047576904, -3.2341203689575195], [2.0545804500579834, -2.339855432510376], [-1.3853970766067505, 1.3434847593307495], [2.835056781768799, -3.1024301052093506], [-1.9354890584945679, 1.8553491830825806], [-0.5917312502861023, 0.49990591406822205], [2.665104389190674, -2.9481394290924072], [-0.8192386031150818, 0.7110495567321777], [2.3866820335388184, -2.6294047832489014], [1.8687490224838257, -2.1366488933563232], [-1.9705138206481934, 1.8934345245361328], [3.027294397354126, -3.2236993312835693], [-2.014310121536255, 1.9314030408859253], [-0.5917312502861023, 0.49990591406822205], [1.9123210906982422, -2.150873899459839], [-0.06651907414197922, -0.01615358702838421], [2.47937273979187, -2.7413527965545654], [2.025391101837158, -2.337613105773926], [-2.0436043739318848, 1.9475136995315552], [2.8817620277404785, -3.109703540802002], [-1.8063997030258179, 1.734178066253662], [-0.3967443108558655, 0.31634753942489624], [2.0342674255371094, -2.2393856048583984]]\n",
      "post-predictions: [[-0.44154849648475647, 0.34076300263404846], [2.9877212047576904, -3.2341203689575195], [2.0545804500579834, -2.339855432510376], [-1.3853970766067505, 1.3434847593307495], [2.835056781768799, -3.1024301052093506], [-1.9354890584945679, 1.8553491830825806], [-0.5917312502861023, 0.49990591406822205], [2.665104389190674, -2.9481394290924072], [-0.8192386031150818, 0.7110495567321777], [2.3866820335388184, -2.6294047832489014], [1.8687490224838257, -2.1366488933563232], [-1.9705138206481934, 1.8934345245361328], [3.027294397354126, -3.2236993312835693], [-2.014310121536255, 1.9314030408859253], [-0.5917312502861023, 0.49990591406822205], [1.9123210906982422, -2.150873899459839], [-0.06651907414197922, -0.01615358702838421], [2.47937273979187, -2.7413527965545654], [2.025391101837158, -2.337613105773926], [-2.0436043739318848, 1.9475136995315552], [2.8817620277404785, -3.109703540802002], [-1.8063997030258179, 1.734178066253662], [-0.3967443108558655, 0.31634753942489624], [2.0342674255371094, -2.2393856048583984], [-0.22666427493095398, 0.11954525113105774], [2.4526612758636475, -2.727910280227661], [1.8116647005081177, -2.0809435844421387], [-1.9270944595336914, 1.8441869020462036], [2.9117937088012695, -3.095384359359741], [-1.9462178945541382, 1.8889330625534058], [-0.3967443108558655, 0.31634753942489624], [1.989784836769104, -2.2141406536102295]]\n",
      "pre-labels: []\n",
      "label: tensor([1, 0, 0, 1, 0, 1, 1, 0])\n",
      "post-labels: [1, 0, 0, 1, 0, 1, 1, 0]\n",
      "pre-labels: [1, 0, 0, 1, 0, 1, 1, 0]\n",
      "label: tensor([1, 0, 0, 1, 0, 1, 1, 0])\n",
      "post-labels: [1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0]\n",
      "pre-labels: [1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0]\n",
      "label: tensor([1, 0, 0, 1, 0, 1, 1, 0])\n",
      "post-labels: [1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0]\n",
      "pre-labels: [1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0]\n",
      "label: tensor([1, 0, 0, 1, 0, 1, 1, 0])\n",
      "post-labels: [1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "#full_dataset = get_dataset(dataset='civilcomments', download=False, root_dir = './wilds/data')\n",
    "\n",
    "predict_augmented_labels(\"nlp_random_word_delete\"\n",
    "                            , \"civilcomments\"\n",
    "                            , \"ERM\"\n",
    "                            , OUTPUT_PATH\n",
    "                            , full_dataset\n",
    "                            , num_samples = 4\n",
    "                            , aug_char_min = 1\n",
    "                            , aug_char_max = 1\n",
    "                            , aug_char_p = 1\n",
    "                            , aug_word_min = 1\n",
    "                            , aug_word_max = None\n",
    "                            , aug_word_p = 0.1\n",
    "                            , aug_sentence_min = 1\n",
    "                            , aug_sentence_max = None\n",
    "                            , aug_sentence_p = 0.1\n",
    "                            , min_char = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0806791  -0.0148583 ]\n",
      " [ 2.18063664 -2.48713326]\n",
      " [ 1.78217852 -2.05658937]\n",
      " ...\n",
      " [ 0.89004332 -1.01642132]\n",
      " [-0.26282683  0.16945408]\n",
      " [ 0.52655756 -0.67368942]]\n",
      "(535128, 2)\n",
      "[1 0 0 ... 0 0 0]\n",
      "(535128,)\n",
      "[[0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0]\n",
      " [0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0]\n",
      " [0 0 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0]]\n",
      "(535128, 17)\n"
     ]
    }
   ],
   "source": [
    "file = OUTPUT_PATH + 'civilcomments_ERM_predictions_optimized_params_four_samples/raw/nlp_random_word_delete.npy'\n",
    "\n",
    "with open(file, 'rb') as aug_file:\n",
    "    aug_logit_predictions = np.load(aug_file, allow_pickle = True)\n",
    "    aug_labels = np.load(aug_file, allow_pickle = True)\n",
    "    metadata = np.load(aug_file, allow_pickle = True)\n",
    "    \n",
    "print(aug_logit_predictions)\n",
    "print(np.shape(aug_logit_predictions))\n",
    "\n",
    "print(aug_labels)\n",
    "print(np.shape(aug_labels))\n",
    "\n",
    "print(metadata[:12])\n",
    "print(np.shape(metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_4_sample_corrections_corruptions(aug_pred_file, original_pred_file):\n",
    "    \"\"\"\n",
    "    operating under data structure of:\n",
    "        predictions = [logit_input_1_sample_1\n",
    "                      , logit_input_1_sample_2\n",
    "                      , logit_input_1_sample_3\n",
    "                      , logit_input_1_sample_4\n",
    "                      , ...]\n",
    "                      \n",
    "        labels = [sample_1_label\n",
    "                  , sample_1_label\n",
    "                  , sample_1_label\n",
    "                  , sample_1_label\n",
    "                  , sample_2_label\n",
    "                  , ...]\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    aug_logit_predictions = []\n",
    "    aug_classified_predictions = []\n",
    "    \n",
    "    original_logit_predictions = []\n",
    "    original_classified_predictions = []\n",
    "    \n",
    "    aug_labels = []\n",
    "    original_labels = []\n",
    "    \n",
    "    with open(aug_pred_file, 'rb') as aug_file:\n",
    "        aug_logit_predictions = np.load(aug_file, allow_pickle = True)\n",
    "        aug_labels = np.load(aug_file, allow_pickle = True)\n",
    "    \n",
    "    with open(original_pred_file, 'rb') as original_file:\n",
    "        original_logit_predictions = np.load(original_file, allow_pickle = True)\n",
    "        original_labels = np.load(original_file, allow_pickle = True)\n",
    "        \n",
    "    assert aug_labels.all() == original_labels.all(), \"Test set labels do not match between aug. and original.\"\n",
    "    assert aug_labels.size == original_labels.size\n",
    "    \n",
    "    for prediction in aug_logit_predictions:\n",
    "        index_prediction = np.argmax(prediction).tolist()\n",
    "        \n",
    "        aug_classified_predictions.append(index_prediction)\n",
    "    \n",
    "    for prediction in original_logit_predictions:\n",
    "        index_prediction = np.argmax(prediction).tolist()\n",
    "        \n",
    "        original_classified_predictions.append(index_prediction)\n",
    "    \n",
    "    \n",
    "    corrections_count = 0\n",
    "    corrections_indices = []\n",
    "    \n",
    "    corruptions_count = 0\n",
    "    corruptions_indices = []\n",
    "    \n",
    "    for i in range(len(aug_labels)):\n",
    "        label = original_labels[i]\n",
    "        original_pred = original_classified_predictions[i]\n",
    "        aug_pred = aug_classified_predictions[i]\n",
    "        \n",
    "        \n",
    "        # if corruption\n",
    "        if (original_pred == label and aug_pred != label):\n",
    "            corruptions_count += 1\n",
    "            corruptions_indices.append(i)\n",
    "        \n",
    "        # if correction\n",
    "        elif (original_pred != label and aug_pred == label):\n",
    "            corrections_count += 1\n",
    "            corrections_indices.append(i)\n",
    "    \n",
    "    corrections_corruptions_dict = {\"num_corrections\": corrections_count\n",
    "                                    , \"num_corruptions\": corruptions_count\n",
    "                                   , \"corrections_indices\": corrections_indices\n",
    "                                   , \"corruptions_indices\": corruptions_indices}\n",
    "    \n",
    "    return corrections_corruptions_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
