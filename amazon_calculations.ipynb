{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU:2\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Set-up: Import numpy and assign GPU\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/local/helenl/.cache/'\n",
    "os.environ['PYTORCH_TRANSFORMERS_CACHE'] = '/local/helenl/.cache/'\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from gpu_utils import restrict_GPU_pytorch\n",
    "from helenl_utils import *\n",
    "from path_stop import *\n",
    "\n",
    "restrict_GPU_pytorch('2')\n",
    "\n",
    "# Load Amazon WILDS pre-trained model\n",
    "\n",
    "import statistics\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import json\n",
    "import random\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import argparse\n",
    "import pdb\n",
    "\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char_min: 1\n",
      "word_p: 0.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100050"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#full_dataset = get_dataset(dataset=\"civilcomments\", download=False, root_dir = './wilds/data')\n",
    "\n",
    "eval_dataset = get_eval_dataset(\"amazon\", \"ERM\")\n",
    "\n",
    "len(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPECTED_PREDICTION_COUNT = 100050\n",
    "EXPECTED_LABEL_COUNT = 100050\n",
    "\n",
    "erroneous_raw_files = {}\n",
    "\n",
    "directory_path = OUTPUT_PATH + \"amazon_ERM_predictions_optimized_params/raw/\"\n",
    "\n",
    "for amazon_file in os.listdir(directory_path):\n",
    "    amazon_file_path = directory_path + amazon_file\n",
    "\n",
    "    with open(amazon_file_path, 'rb') as file:\n",
    "        logit_predictions = np.load(file, allow_pickle = True)\n",
    "        labels = np.load(file, allow_pickle = True)\n",
    "        \n",
    "\n",
    "    actual_prediction_count = len(logit_predictions)\n",
    "    actual_label_count = len(labels)\n",
    "    \n",
    "    if actual_prediction_count != EXPECTED_PREDICTION_COUNT:\n",
    "        erroneous_raw_files[amazon_file] = (actual_prediction_count, actual_label_count)\n",
    "    elif actual_label_count != EXPECTED_LABEL_COUNT:\n",
    "        erroneous_raw_files[amazon_file] = (actual_prediction_count, actual_label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bert.npy': (400200, 100050)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "erroneous_raw_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp_wordnet_synonym.npy\n",
      "400200\n",
      "100050\n",
      "nlp_random_token_split.npy\n",
      "400200\n",
      "100050\n",
      "nlp_ocr.npy\n",
      "400200\n",
      "100050\n",
      "nlp_random_word_delete.npy\n",
      "400200\n",
      "100050\n",
      "bert.npy\n",
      "400200\n",
      "100050\n",
      "nlp_random_char_swap.npy\n",
      "400200\n",
      "100050\n",
      "nlp_random_char_deletion.npy\n",
      "400200\n",
      "100050\n",
      "nlp_ppdb_synonym.npy\n",
      "400200\n",
      "100050\n"
     ]
    }
   ],
   "source": [
    "directory_path = OUTPUT_PATH + \"amazon_ERM_predictions_optimized_params/with_bert/\"\n",
    "\n",
    "for amazon_file in os.listdir(directory_path):\n",
    "    amazon_file_path = directory_path + amazon_file\n",
    "\n",
    "    with open(amazon_file_path, 'rb') as file:\n",
    "        logit_predictions = np.load(file, allow_pickle = True)\n",
    "        labels = np.load(file, allow_pickle = True)\n",
    "\n",
    "    actual_prediction_count = len(logit_predictions)\n",
    "    actual_label_count = len(labels)\n",
    "\n",
    "    print(amazon_file)\n",
    "    print(actual_prediction_count)\n",
    "    print(actual_label_count)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPECTED_PREDICTION_COUNT = 400200\n",
    "EXPECTED_LABEL_COUNT = 400200\n",
    "\n",
    "erroneous_raw_files = {}\n",
    "\n",
    "directory_path = OUTPUT_PATH + \"amazon_ERM_predictions_optimized_params_four_samples/raw/\"\n",
    "\n",
    "for amazon_file in os.listdir(directory_path):\n",
    "    amazon_file_path = directory_path + amazon_file\n",
    "\n",
    "    with open(amazon_file_path, 'rb') as file:\n",
    "        logit_predictions = np.load(file, allow_pickle = True)\n",
    "        labels = np.load(file, allow_pickle = True)\n",
    "        \n",
    "\n",
    "    actual_prediction_count = len(logit_predictions)\n",
    "    actual_label_count = len(labels)\n",
    "    \n",
    "    if actual_prediction_count != EXPECTED_PREDICTION_COUNT:\n",
    "        erroneous_raw_files[amazon_file] = (actual_prediction_count, actual_label_count)\n",
    "    elif actual_label_count != EXPECTED_LABEL_COUNT:\n",
    "        erroneous_raw_files[amazon_file] = (actual_prediction_count, actual_label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "erroneous_raw_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
