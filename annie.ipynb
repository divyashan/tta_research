{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfad1e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import time\n",
    "import json\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.spatial.distance import cdist, cosine, euclidean\n",
    "from scipy.stats import ttest_ind, ttest_1samp\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from gpu_utils import restrict_GPU_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1b5ec5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU:0\n"
     ]
    }
   ],
   "source": [
    "restrict_GPU_pytorch('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ace3eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list of predictions (prob of moderation) for a list of examples and a given model. \n",
    "# allExamples: list of strings \n",
    "# model: fastText model object\n",
    "def getModelPredsHelper(allExamples, model):\n",
    "    exampleList = [x.replace('\\n', ' ') for x in allExamples]\n",
    "    preds = model.predict(exampleList)\n",
    "    preds_int = np.array([1 if 'positive' in p[0] else 0 for p in preds[0]])\n",
    "    preds_prob = np.array([p[0] for p in preds[1]])\n",
    "    probs = np.array([1 - p if preds_int[i] == 0 else p for (i,p) in enumerate(preds_prob)])\n",
    "    probs = [np.round(p,3) for p in probs]\n",
    "    return probs\n",
    "\n",
    "# vec: query vector (list of floats)\n",
    "# all_vecs: pool of all vectors from which neighbors are retrieved (list of list of floats)\n",
    "# comments: comments corresponding to the vectors in all_vecs (list of strings)\n",
    "# n: number of neighbors to return (int)\n",
    "# return_idx: if true, returns indices of nearest neighbors instead of the actual comments (bool)\n",
    "def getKNNFromVector(vec, all_vecs, comments, n=30, return_idx=False):\n",
    "    dist_vec = cdist(vec, all_vecs, 'cosine')\n",
    "    top_vec_idx = np.argsort(dist_vec[0])[1:n+1]\n",
    "    if return_idx: return top_vec_idx\n",
    "    top_comments = np.array(comments)[top_vec_idx]\n",
    "    top_comments = [c for c in top_comments]\n",
    "    return top_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27eb67f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FPATH = '../data/reddit/'\n",
    "MODEL_FPATH = '../models/reddit/'\n",
    "USE_PATH = '../Dev/tf_hub/universal-sentence-encoder_4/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9c46e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = ['askscience', 'conspiracy', 'funny', 'hillaryclinton', 'history']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88050177",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a995422",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments = pd.read_csv(DATA_FPATH + 'all_comments_df')\n",
    "embed = hub.load(USE_PATH);\n",
    "all_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b7c9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = 'funny'\n",
    "comments = all_comments[all_comments.subreddit == sub].body.values\n",
    "labels = all_comments[all_comments.subreddit == sub].moderated.values\n",
    "vecs = np.array(embed(comments))\n",
    "subreddit_model = fasttext.load_model(MODEL_FPATH + \"%s_model.bin\" % sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f92ee1a",
   "metadata": {},
   "source": [
    "## Nearest Neighbor Search for Example Set Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7949c4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = \"\"\"\n",
    "poor snowflake do you need a safe space\n",
    "\"\"\"\n",
    "\n",
    "top_vec_idx = getKNNFromVector(embed([seed]), vecs, comments, n=50, return_idx=True)\n",
    "knn_vecs = vecs[top_vec_idx]\n",
    "knn_comments = comments[top_vec_idx]\n",
    "num_clusters = 4\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(knn_vecs)\n",
    "\n",
    "for cluster in range(num_clusters):\n",
    "    cluster_comments = knn_comments[np.where(kmeans.labels_ == cluster)]\n",
    "    \n",
    "    print('===================== CLUSTER %i =========================' % cluster)\n",
    "    for c in cluster_comments:\n",
    "        print(c, end='\\n------------------------\\n')\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305d6727",
   "metadata": {},
   "source": [
    "### Next steps: Annie\n",
    "\n",
    "Example search & generation has three main steps (though they may be intertwined in some ways): 1) getting similar examples, 2) clustering or organizing them in some way 3) visualizing the result.  Right now we're doing 1) with euclidean distance in embedding space of the USE, 2) with k-means clustering, and 3) with just printing out the examples. \n",
    "\n",
    "We can think about ways to improve each of these parts.  To start, let's explore 1).  Right now we are getting similar examples with the USE (the model we load from TF Hub).  How does this compare to a different embedding model?  Here are some we can try: \n",
    "* BERT: https://huggingface.co/bert-base-uncased\n",
    "* RoBERTa: https://huggingface.co/roberta-base\n",
    "* XLNet: https://huggingface.co/xlnet-base-cased\n",
    "\n",
    "Later, we could also think about fine-tuning some of these embeddings to be better suited to our task/data.  Sample code for loading BERT and getting embeddings for a sample of sentences: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b2d08d",
   "metadata": {},
   "source": [
    "### To do: \n",
    "* modify `getKNNFromVector` to take a particular embedding name (e.g., 'BERT') and compute distances in that embedding space. \n",
    "* for a handful of seed sentences, get the nearest neighbors and print them out as above in each of the different embedding spaces. Qualitatively note differences you notice among what is returned as similar. Do some seem better or worse?  Are there noticeable differences? \n",
    "* for an easy 2 or 3D projection, you can try loading the data into the embedding projector: https://projector.tensorflow.org/  You may want to just do a particular example and its 100 nearest neighbors or something (rather than all the data). You'll have to save the embeddings and sentences as TSV files and then load them in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d6f824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "from transformers import DistilBertTokenizerFast, DistilBertModel\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# bert_model.cuda()\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93e1461",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c03f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03d2c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sentences = [\"Hello, my dog is cute\", \"another sample sentence\"]\n",
    "tokenized_sentences = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"pt\")['input_ids']\n",
    "tokenized_sentences = tokenized_sentences.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0289197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750cdb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(tokenized_sentences)['last_hidden_state'].cpu().detach().numpy()\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df95fafa",
   "metadata": {},
   "source": [
    "#### BERT embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb43f4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2295b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "for i in range(len(comments)):\n",
    "    model(tokenizer(list(comments[i]), padding=True, truncation=True, return_tensors=\"pt\")['input_ids'].cuda())['last_hidden_state'].cpu().detach().numpy()\n",
    "    if i % 100 == 0:\n",
    "        print(f'{i} of 34347')\n",
    "t1 = time.time()\n",
    "print(f'{t1 - t0} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416e5bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_comments = tokenizer(list(comments), padding=True, truncation=True, return_tensors=\"pt\")['input_ids']\n",
    "tokenized_comments = tokenized_comments.cuda()\n",
    "tokenized_comments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1b4095",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_output = model(tokenized_comments)['last_hidden_state'].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3bbff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd41a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "for i in range(len(comments)):\n",
    "    tokenized_comment = tokenizer(list(comments[i]), padding=True, truncation=True, return_tensors=\"pt\")['input_ids'].cuda()\n",
    "    output = bert_model(tokenized_comment)['pooler_output'].detach()\n",
    "    if i % 1000 == 0:\n",
    "        print(f'{i} of 34347')\n",
    "t1 = time.time()\n",
    "print(f'{t1 - t0} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee17ad94",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_output = bert_model(tokenized_comment)['pooler_output'].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375b8ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97534e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 1000\n",
    "size = 34347//num_iterations\n",
    "for i in range(num_iterations):\n",
    "    tokenized_comments = tokenizer(list(comments[size*i : min(size*(i+1), 34347)]), padding=True, truncation=True, return_tensors=\"pt\")['input_ids'].cuda()\n",
    "    bert_output = bert_model(tokenized_comments)['pooler_output'].detach().shape()\n",
    "    print(bert_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e780708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comments_bert_model(comments, batch_size):\n",
    "    comments_output = None\n",
    "    num_iterations = 1+len(comments)//batch_size\n",
    "    for i in range(num_iterations):\n",
    "        small_sample_comments = list(comments[batch_size*i : min(batch_size*(i+1), len(comments))])\n",
    "        tokenized_small_sample_comments = tokenizer(small_sample_comments, padding=True, truncation=True, return_tensors=\"pt\")['input_ids']\n",
    "        tokenized_small_sample_comments.cuda()\n",
    "        small_sample_comments_output = model(tokenized_small_sample_comments)['pooler_output'].cpu().detach().numpy()\n",
    "        print(f'{i+1}/{num_iterations} iterations complete')\n",
    "        if comments_output is None:\n",
    "            comments_output = small_sample_comments_output\n",
    "        else:\n",
    "            comments_output = np.concatenate((comments_output, small_sample_comments_output), axis=0)\n",
    "    return comments_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbe0031",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_comments_output = comments_bert_model(comments, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d44b19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc74468",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_comments = tokenizer(list(comments), padding=True, truncation=True, return_tensors=\"pt\")['input_ids']\n",
    "output = bert_model(tokenized_comments) #['pooler_output'].detach().numpy()\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aac277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comments_bert_model_universal_sentence_encoder(comments, batch_size):\n",
    "    comments_output = None\n",
    "    num_iterations = 1+len(comments)//batch_size\n",
    "    for i in range(num_iterations):\n",
    "        small_sample_comments = list(comments[batch_size*i : min(batch_size*(i+1), len(comments))])\n",
    "        tokenized_small_sample_comments = torch.from_numpy(embed(small_sample_comments).numpy()).long()\n",
    "        small_sample_comments_output = bert_model(tokenized_small_sample_comments)['pooler_output'].detach().numpy()\n",
    "        print(f'{i+1}/{num_iterations} iterations complete')\n",
    "        if comments_output is None:\n",
    "            comments_output = small_sample_comments_output\n",
    "        else:\n",
    "            comments_output = np.concatenate((comments_output, small_sample_comments_output), axis=0)\n",
    "    return comments_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6257d3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "universal_comments_output = comments_bert_model_universal_sentence_encoder(comments, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c7a2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = {'BERT': bert_comments_output, 'universal-sentence-encoder_4': embed(comments)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7220ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeds: query comments (list of strings)\n",
    "# comments: comments to compare to (list of strings)\n",
    "# embedding: string corresponding to a particular embedding name (e.g. 'BERT')\n",
    "# n: number of neighbors to return (int)\n",
    "# return_idx: if true, returns indices of nearest neighbors instead of the actual comments (bool)\n",
    "def getKNNFromVector(seeds, comments, embedding, n=30, return_idx=False):\n",
    "    comments_output = outputs[embedding]\n",
    "    if embedding == 'BERT':\n",
    "        tokenized_seeds = tokenizer(seeds, padding=True, truncation=True, return_tensors=\"pt\")['input_ids']\n",
    "    elif embedding == 'universal-sentence-encoder_4':\n",
    "        tokenized_seeds = embed(seeds)\n",
    "    else:\n",
    "        return \"invalid embedding provided\"\n",
    "    \n",
    "    seeds_output = bert_model(tokenized_seeds)['pooler_output'].detach().numpy()\n",
    "    dist_vec = cdist(seeds_output, comments_output, 'cosine')\n",
    "    top_vec_idx = np.argsort(dist_vec[0])\n",
    "    top_comments = np.array(comments[:batch_size*num_iterations])[top_vec_idx]\n",
    "    top_n_comments = [c for c in top_comments[:n]]\n",
    "    return top_n_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cfb3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_embeddings(seeds, comments):\n",
    "    if not isinstance(seeds, list):\n",
    "        print(\"Must input a list of seeds.\")\n",
    "        return\n",
    "    else:\n",
    "        if len(seeds) == 1:\n",
    "            print(f'Testing on the seed \"{decode(seeds[0])}\"')\n",
    "        else:\n",
    "            s = \", \".join([f'\"{decode(seed)}\"' for seed in seeds])\n",
    "            print(f'Testing on the seeds {s}')\n",
    "    \n",
    "    print('\\nResults with universal_sentence_encoder_4:')\n",
    "    try:\n",
    "        embed_results = getKNNFromVector(seeds, comments, n=10, return_idx=False)\n",
    "        for (i, x) in enumerate(embed_results):\n",
    "            print(f'{i+1}. \"{decode(x)}\"')\n",
    "    except:\n",
    "        print(\"something failed\")\n",
    "    \n",
    "    print('\\nResults with BERT:')\n",
    "    try:\n",
    "        bert_results = getKNNFromVector(seeds, comments, embedding=\"BERT\", n=10, return_idx=False)\n",
    "        for (i,x) in enumerate(bert_results):\n",
    "            print(f'{i+1}. \"{decode(x)}\"')\n",
    "    except:\n",
    "        print(\"something failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99dc065",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [\"hello\"]\n",
    "getKNNFromVector(seeds, list(comments), embedding=\"BERT\", n=10, return_idx=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f6f5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [\"hello\"]\n",
    "compare_embeddings(seeds, comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74914459",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [\"\\npoor snowflake do you need a safe space\\n\"]\n",
    "compare_embeddings(seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbf00f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [\"Hello, my dog is cute\", \"another sample sentence\"]\n",
    "compare_embeddings(seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fcb2f1",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f84525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantify_shift(shift_target, example_list, example_list_2, model, labels = None, labels_2 = None, list_names=[\"list 1\", \"list 2\"]): \n",
    "    if shift_target == \"predictions\":\n",
    "        preds_1 = getModelPredsHelper(example_list, model)\n",
    "        preds_2 = getModelPredsHelper(example_list_2, model)\n",
    "        ttest_result = ttest_ind(preds_1, preds_2)\n",
    "        if ttest_result.pvalue >= 0.05: \n",
    "            print(\"Predictions are not significantly different.\")\n",
    "        else: \n",
    "            operator = \"higher\" if ttest_result.statistic > 0 else \"lower\"\n",
    "            print(\"P(moderated) for %s is *%s* than for %s (pval = %.3f)\" % (list_names[0], operator, list_names[1], ttest_result.pvalue))\n",
    "            \n",
    "            \n",
    "    elif shift_target == \"representation\":\n",
    "        vecs_1 = [model.get_sentence_vector(ex) for ex in example_list]\n",
    "        vecs_2 = [model.get_sentence_vector(ex) for ex in example_list_2]\n",
    "        intergroup_diffs = cdist(vecs_1, vecs_2).flatten()\n",
    "        intragroup_diffs = np.concatenate((cdist(vecs_1, vecs_1).flatten(), cdist(vecs_2, vecs_2).flatten()))\n",
    "        ttest_result = ttest_ind(intergroup_diffs, intragroup_diffs)\n",
    "        if ttest_result.statistic > 0 and ttest_result.pvalue < 0.05: \n",
    "            print(\"Representations for %s are significantly different from %s (pval = %f).\" % (list_names[0], list_names[1], ttest_result.pvalue))\n",
    "        else: \n",
    "            print(\"Representations for %s and %s are not significantly different.\" % (list_names[0], list_names[1]))\n",
    "            \n",
    "            \n",
    "    elif shift_target == \"performance\":\n",
    "        preds_1 = getModelPredsHelper(example_list, model)\n",
    "        preds_2 = getModelPredsHelper(example_list_2, model)\n",
    "        perf_1 = np.array([np.round(preds_1[i]) == labels[i] for i in range(len(preds_1))]).astype(int)\n",
    "        perf_2 = np.array([np.round(preds_2[i]) == labels_2[i] for i in range(len(preds_2))]).astype(int)\n",
    "        ttest_result = ttest_ind(perf_1, perf_2)\n",
    "        if ttest_result.pvalue >= 0.05: \n",
    "            print(\"Model performance on %s and %s is not significantly different.\" % (list_names[0], list_names[1]))\n",
    "        else: \n",
    "            operator = \"higher\" if ttest_result.statistic > 0 else \"lower\"\n",
    "            print(\"Model performance on %s is *%s* than for %s (pval = %.3f)\" % (list_names[0], operator, list_names[1], ttest_result.pvalue))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
