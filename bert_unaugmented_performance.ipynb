{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7686186d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU:1\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Set-up: Import numpy and assign GPU\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from gpu_utils import restrict_GPU_pytorch\n",
    "restrict_GPU_pytorch('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf6c066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Amazon WILDS pre-trained model\n",
    "import statistics\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import argparse\n",
    "import pdb\n",
    "\n",
    "from wilds import get_dataset\n",
    "from wilds.common.data_loaders import get_train_loader, get_eval_loader\n",
    "from wilds.common.grouper import CombinatorialGrouper\n",
    "from transforms_helenl import initialize_transform, getBertTokenizer\n",
    "\n",
    "sys.path.insert(0, './wilds/examples/')\n",
    "from algorithms.initializer import initialize_algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33b1764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75001118",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "        \n",
    "#config_dict = pickle.load(open('amazon_config.txt', 'rb'))\n",
    "\n",
    "\n",
    "\n",
    "full_dataset = get_dataset(dataset='civilcomments', download=False, root_dir = './wilds/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a476500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_augmented_labels(transform_name):\n",
    "    \n",
    "    # Grab config generated by WILDS library from txt file\n",
    "    infile = open('civilcomments_config.txt','rb')\n",
    "    new_dict = vars(pickle.load(infile))\n",
    "    infile.close() \n",
    "\n",
    "    # Modify config with intended transformation\n",
    "    new_dict['transform'] = transform_name\n",
    "\n",
    "    # Create config with ERM algorithm\n",
    "    config = Namespace(**new_dict)\n",
    "    config.algorithm = 'ERM'\n",
    "\n",
    "    \n",
    "    # Generate training and evaluation transforms\n",
    "    train_transform = initialize_transform(transform_name=config.transform,\n",
    "                                      config=config,\n",
    "                                      dataset=full_dataset,\n",
    "                                      is_training=True)\n",
    "    eval_transform = initialize_transform(transform_name=config.transform,\n",
    "                                      config=config,\n",
    "                                      dataset=full_dataset,\n",
    "                                      is_training=False)\n",
    "    \n",
    "    \n",
    "    # Prepare training data, loader, and grouper\n",
    "    train_data = full_dataset.get_subset('train', transform=train_transform)\n",
    "    train_loader = get_train_loader('standard', train_data, batch_size=64)\n",
    "    train_grouper = CombinatorialGrouper(dataset=full_dataset, groupby_fields=config.groupby_fields)\n",
    "    \n",
    "        # Prepare training data, loader, and grouper\n",
    "        \n",
    "    eval_data = full_dataset.get_subset('test', transform=eval_transform)\n",
    "    eval_loader = get_eval_loader('standard', eval_data, batch_size = 64)\n",
    "    \n",
    "    # CODE TAKEN FROM WILDS TRAINING SCRIPTS:\n",
    "    datasets = defaultdict(dict)\n",
    "    for split in full_dataset.split_dict.keys():\n",
    "        if split=='train':\n",
    "            transform = train_transform\n",
    "            verbose = True\n",
    "        elif split == 'val':\n",
    "            transform = eval_transform\n",
    "            verbose = True\n",
    "        else:\n",
    "            transform = eval_transform\n",
    "            verbose = False\n",
    "        # Get subset\n",
    "        datasets[split]['dataset'] = full_dataset.get_subset(\n",
    "            split,\n",
    "            frac=config.frac,\n",
    "            transform=transform)\n",
    "\n",
    "        if split == 'train':\n",
    "            datasets[split]['loader'] = get_train_loader(\n",
    "                loader=config.train_loader,\n",
    "                dataset=datasets[split]['dataset'],\n",
    "                batch_size=config.batch_size,\n",
    "                uniform_over_groups=config.uniform_over_groups,\n",
    "                grouper=train_grouper,\n",
    "                distinct_groups=config.distinct_groups,\n",
    "                n_groups_per_batch=config.n_groups_per_batch,\n",
    "            **config.loader_kwargs)\n",
    "        else:\n",
    "            datasets[split]['loader'] = get_eval_loader(\n",
    "                loader=config.eval_loader,\n",
    "                dataset=datasets[split]['dataset'],\n",
    "                grouper=train_grouper,\n",
    "                batch_size=config.batch_size,\n",
    "                **config.loader_kwargs)\n",
    "\n",
    "    # Set fields\n",
    "    datasets[split]['split'] = split\n",
    "    datasets[split]['name'] = full_dataset.split_names[split]\n",
    "    datasets[split]['verbose'] = verbose\n",
    "\n",
    "    # Loggers\n",
    "    # datasets[split]['eval_logger'] = BatchLogger(\n",
    "    #     os.path.join(config.log_dir, f'{split}_eval.csv'), mode=mode, use_wandb=(config.use_wandb and verbose))\n",
    "    # datasets[split]['algo_logger'] = BatchLogger(\n",
    "    #     os.path.join(config.log_dir, f'{split}_algo.csv'), mode=mode, use_wandb=(config.use_wandb and verbose))\n",
    "\n",
    "    print(transform_name)\n",
    "    print(\"initialize model\")\n",
    "    # Initiate model and run on training set\n",
    "    model = initialize_algorithm(config, datasets, train_grouper)\n",
    "    model.model.cuda()\n",
    "    \n",
    "    print(\"initialization complete\")\n",
    "    \n",
    "    \n",
    "    print(\"generating predictions\")\n",
    "    it = iter(eval_loader)\n",
    "    predictions = []\n",
    "    true_values = []\n",
    "    \n",
    "    for batch in tqdm(it):\n",
    "        \n",
    "        #pdb.set_trace()\n",
    "        raw_pred = model.model(batch[0].cuda()).cpu().detach().numpy().tolist()\n",
    "        #softmax_prediction = sp.special.softmax(raw_pred.cpu().detach().numpy()).tolist()\n",
    "        predictions.append(raw_pred)\n",
    "        true_values.extend(batch[1].tolist())\n",
    "    \n",
    "\n",
    "    print(\"writing predictions\")\n",
    "    file_name = './ERM_predictions/' + transform_name + \".npy\"\n",
    "    \n",
    "    with open(file_name, 'wb+') as file:\n",
    "        np.save(file, predictions)\n",
    "        np.save(file, true_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f64c1ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(prediction_file):\n",
    "    logit_predictions = []\n",
    "    labels = []\n",
    "    \n",
    "    with open(prediction_file, 'rb') as file:\n",
    "        logit_predictions = np.load(file, allow_pickle = True)\n",
    "        labels = np.load(file, allow_pickle = True)\n",
    "    \n",
    "    '''\n",
    "    print(len(logit_predictions[0]))\n",
    "    print(type(logit_predictions))\n",
    "    print(logit_predictions)\n",
    "    '''\n",
    "    \n",
    "    softmaxed_predictions = []\n",
    "    \n",
    "    for pred in logit_predictions[0]:\n",
    "        softmax = sp.special.softmax(pred).tolist()\n",
    "        softmaxed_predictions.append(softmax[1])\n",
    "        \n",
    "    #plt.hist(logit_predictions)\n",
    "    #plt.hist(softmaxed_predictions)\n",
    "    \n",
    "    #return sklearn.metrics.roc_auc_score(labels[:64], softmaxed_predictions)\n",
    "\n",
    "    classified_predictions = []\n",
    "    for prediction in logit_predictions:\n",
    "        index_prediction = np.argmax(prediction, axis = 1).tolist()\n",
    "        \n",
    "        classified_predictions.extend(index_prediction)\n",
    "    \n",
    "    unique, counts = np.unique(np.array(classified_predictions), return_counts = True)\n",
    "    print(unique, counts)\n",
    "    print(len(labels))\n",
    "        \n",
    "    print(\"PREDICTIONS:\", len(classified_predictions))\n",
    "    #print(\"PREDICTIONS SAMPLE:\", classified_predictions[0])\n",
    "    print(\"TRUE VALUES:\", len(labels))\n",
    "    #print(\"TRUE VALUES SAMPLE:\", len(labels[0]))\n",
    "    #print(type(true_values))\n",
    "    \n",
    "    score = sklearn.metrics.accuracy_score(labels, classified_predictions)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85084f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(prediction_file):\n",
    "    logit_predictions = []\n",
    "    \n",
    "    with open(prediction_file, 'rb') as file:\n",
    "        logit_predictions = np.load(file)\n",
    "        true_values = np.load(file)\n",
    "        \n",
    "    print(true_values)\n",
    "    classified_predictions = []\n",
    "    for prediction in logit_predictions:\n",
    "        if prediction[0][0] > prediction[0][1]:\n",
    "            classified_predictions.append(0)\n",
    "            \n",
    "        else:\n",
    "            classified_predictions.append(1)\n",
    "    \n",
    "    score = sklearn.metrics.recall_score(true_values, classified_predictions)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2b2b98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert\n",
      "initialize model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertClassifier: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertClassifier were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialization complete\n",
      "generating predictions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b208557fff4b0d87c36bf33942bfa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/helenl/.conda/envs/tta/lib/python3.6/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    }
   ],
   "source": [
    "predict_augmented_labels('bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c597c95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1] [ 14589 119193]\n",
      "133782\n",
      "PREDICTIONS: 133782\n",
      "TRUE VALUES: 133782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2053116263772406"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "calculate_accuracy('./ERM_predictions/bert.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c778aa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_distribution(prediction_file):\n",
    "    logit_predictions = []\n",
    "    labels = []\n",
    "    \n",
    "    with open(prediction_file, 'rb') as file:\n",
    "        logit_predictions = np.load(file, allow_pickle = True)\n",
    "        labels = np.load(file, allow_pickle = True)\n",
    "    \n",
    "    softmaxed_predictions = []\n",
    "    \n",
    "    for pred in logit_predictions[0]:\n",
    "        softmax = sp.special.softmax(pred).tolist()\n",
    "        softmaxed_predictions.append(softmax[1])\n",
    "        \n",
    "    plt.hist(logit_predictions)\n",
    "    plt.hist(softmaxed_predictions)\n",
    "    \n",
    "    #return sklearn.metrics.roc_auc_score(labels[:64], softmaxed_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17564690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_prediction_distribution('./ERM_predictions/bert.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3dd97eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_roc_auc_score(prediction_file):\n",
    "    logit_predictions = []\n",
    "    labels = []\n",
    "    \n",
    "    with open(prediction_file, 'rb') as file:\n",
    "        logit_predictions = np.load(file, allow_pickle = True)\n",
    "        labels = np.load(file, allow_pickle = True)\n",
    "    \n",
    "    softmaxed_predictions = []\n",
    "    \n",
    "    for batch in logit_predictions:\n",
    "        for pred in batch:\n",
    "            softmax = sp.special.softmax(pred).tolist()\n",
    "            softmaxed_predictions.append(softmax[1])\n",
    "        \n",
    "    \n",
    "    return sklearn.metrics.roc_auc_score(labels, softmaxed_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d953b54a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [64, 133782]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-271af4deb7e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcalculate_roc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./ERM_predictions/bert.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-1718f55dba50>\u001b[0m in \u001b[0;36mcalculate_roc_auc_score\u001b[0;34m(prediction_file)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoftmaxed_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/local/helenl/.conda/envs/tta/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/helenl/.conda/envs/tta/lib/python3.6/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    543\u001b[0m                                              max_fpr=max_fpr),\n\u001b[1;32m    544\u001b[0m                                      \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m                                      sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    546\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# multilabel-indicator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         return _average_binary_score(partial(_binary_roc_auc_score,\n",
      "\u001b[0;32m/local/helenl/.conda/envs/tta/lib/python3.6/site-packages/sklearn/metrics/_base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/helenl/.conda/envs/tta/lib/python3.6/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     fpr, tpr, _ = roc_curve(y_true, y_score,\n\u001b[0;32m--> 331\u001b[0;31m                             sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    332\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmax_fpr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmax_fpr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/helenl/.conda/envs/tta/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/helenl/.conda/envs/tta/lib/python3.6/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    912\u001b[0m     \"\"\"\n\u001b[1;32m    913\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0;32m--> 914\u001b[0;31m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m     \u001b[0;31m# Attempt to drop thresholds corresponding to points in between and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/helenl/.conda/envs/tta/lib/python3.6/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/helenl/.conda/envs/tta/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 320\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [64, 133782]"
     ]
    }
   ],
   "source": [
    "calculate_roc_auc_score('./ERM_predictions/bert.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4974605f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
