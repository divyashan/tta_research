import torch
import os
import sys
from torchvision import datasets
from torchvision import transforms
import pickle
import random
import numpy as np
from tqdm.notebook import tqdm
import sklearn

from collections import defaultdict

from transforms_helenl import initialize_transform, getBertTokenizer
from wilds import get_dataset
from wilds.common.data_loaders import get_train_loader, get_eval_loader
from wilds.common.grouper import CombinatorialGrouper

sys.path.insert(0, './wilds/examples/')
os.environ["MODEL_DIR"] = '/local/helenl/helenl/models/'
model_dir = '/local/helenl/helenl/models'

from algorithms.initializer import initialize_algorithm





class Namespace:
    def __init__(self, **kwargs):
        self.__dict__.update(kwargs)



def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        


"""
Generates file containing predictions, label, and metadata after a specific augmentation is 
applied to evaluation dataset inputs.


Parameters: 
    transform_name (str): Name of transformation used to augment data input. 
                          Must be supported in transforms_helenl.py.
                          
    dataset_name (str)  : Name of dataset to be used for predictions. 
                          Currently accepted inputs are: 'amazon' and 'civilcomments'. There must also exist a .txt file                             containing the model configuration with the dataset name in the ./model_configs/ directory and a                           .pth file containing the model's pre-trained weights in the ./best_models/ directory.
 
                      
                      
                      
   algorithm_name (str) : Name of algorithm to be used for predictions. 
                          Currently accepted inputs are: 'ERM' and 'GroupDRO'. There must also exist a .txt file                                     containing the model configuration with the algorithm name in the ./model_configs/ directory and a                         .pth file containing the model's pre-trained weights in the ./best_models/ directory.
                          
                          
    saved_file_dir (str): Path of directory that predictions .npy file will be saved in. Directory must already exist prior                           to running this function.
    
    
    num_samples (int)   : Number of samples to be generated by applying augmentation to given input. 
                          A value greater than the possible number of unique augmentations created by applying
                          described augmentation to input will result in duplicate samples for prediction.
                          Default value of 1.
                          
    aug_char_min (int)  : Minimum number of characters to be augmented using a character-based augmentation.
                          Default value of 1.
    
    aug_char_max (int)  : Maximum number of characters to be augmented using a character-based augmentation.
                          Default value of 1.
    
    aug_char_p (int)    : Percent of characters to be augmented using a character-based augmentation.
                          Default value of 1.
    
    aug_word_min (int)  : Minimum number of words to be augmented using a character-based or word-based augmentation.
                          Default value of 1.
    
    aug_word_max (int)  : Maximum number of words to be augmented using a character-based or word-based augmentation.
                          Default value of None.
    
    aug_word_p (int)    : Percent of words to be augmented using a character-based or word-based augmentation.
                          Default value of 0.1.
                          
    min_char (int)      : Minimum number of characters in a word in order for word to be augmented with 
                          character-based augmentation. If a word with fewer than min_char characters is
                          selected for augmentation, the augmentation will not be applied and the word will
                          remain unchanged.


Returns:

    eval_data_info (tup) :Tuple with first index containing list of indices used to form evaluation dataset from original                             dataset and second dataset containing entire original dataset.

"""


def predict_augmented_labels(transform_name
                            , dataset_name
                            , algorithm_name
                            , saved_file_dir
                            , full_dataset
                            , num_samples = 1
                            , aug_char_min = 1
                            , aug_char_max = 1
                            , aug_char_p = 1
                            , aug_word_min = 1
                            , aug_word_max = None
                            , aug_word_p = 0.1
                            , aug_sentence_min = 1
                            , aug_sentence_max = None
                            , aug_sentence_p = 0.1
                            , min_char = 4):

    
    assert dataset_name in ['amazon', 'civilcomments'], "Dataset not supported."
    assert os.path.isdir(saved_file_dir), "Directory for predictions file to be saved does not exist."
    
    set_seed(0)
    
    # Grab config generated by WILDS library from txt file

    config_file = './model_configs/{}_{}_config.txt'.format(dataset_name, algorithm_name)
    
    assert os.path.isfile(config_file), "model configuration file not found in ./model_configs/ directory"
        
    infile = open(config_file,'rb')
    new_dict = vars(pickle.load(infile))
    infile.close() 
    

    # Modify config with intended transformation
    new_dict['transform'] = transform_name
    new_dict['batch_size'] = 8
    new_dict['unlabeled_batch_size'] = 8
    new_dict['loader_kwargs']['num_workers'] = 4


    # Create config with ERM algorithm
    config = Namespace(**new_dict)
    

    # Generate training and evaluation transforms
    print("predict_augmented_labels() num_samples:", num_samples)
    train_transform = initialize_transform(transform_name=config.transform
                                          , config=config
                                          , dataset=full_dataset
                                          , is_training=True
                                          , num_samples = num_samples
                                          , aug_char_min = aug_char_min
                                          , aug_char_max = aug_char_max
                                          , aug_char_p = aug_char_p
                                          , aug_word_min = aug_word_min
                                          , aug_word_max = aug_word_max
                                          , aug_word_p = aug_word_p
                                          , min_char = min_char)
    
    # Prepare training data, loader, and grouper
    train_data = full_dataset.get_subset('train', transform=train_transform)
    train_loader = get_train_loader('standard', train_data, batch_size=64)
    train_grouper = CombinatorialGrouper(dataset=full_dataset, groupby_fields=config.groupby_fields)
    
    # Prepare training data, loader, and grouper
    eval_transform = initialize_transform(transform_name=config.transform
                                          , config=config
                                          , dataset=full_dataset
                                          , is_training=False
                                          , num_samples = num_samples
                                          , aug_char_min = aug_char_min
                                          , aug_char_max = aug_char_max
                                          , aug_char_p = aug_char_p
                                          , aug_word_min = aug_word_min
                                          , aug_word_max = aug_word_max
                                          , aug_word_p = aug_word_p
                                          , aug_sentence_min = aug_sentence_min
                                          , aug_sentence_max = aug_sentence_max
                                          , aug_sentence_p = aug_sentence_p
                                          , min_char = min_char
                                        )
    
    
    eval_data = full_dataset.get_subset('test', transform=eval_transform)


    eval_loader = get_eval_loader('standard', eval_data, batch_size = 8)
    
    # CODE TAKEN FROM WILDS TRAINING SCRIPTS:
    datasets = defaultdict(dict)
    for split in full_dataset.split_dict.keys():
        if split=='train':
            transform = train_transform
            verbose = True
        elif split == 'val':
            transform = eval_transform
            verbose = True
        else:
            transform = eval_transform
            verbose = False
        # Get subset
        datasets[split]['dataset'] = full_dataset.get_subset(
            split,
            frac=config.frac,
            transform=transform)

        if split == 'train':
            datasets[split]['loader'] = get_train_loader(
                loader=config.train_loader,
                dataset=datasets[split]['dataset'],
                batch_size=config.batch_size,
                uniform_over_groups=config.uniform_over_groups,
                grouper=train_grouper,
                distinct_groups=config.distinct_groups,
                n_groups_per_batch=config.n_groups_per_batch,
            **config.loader_kwargs)
        else:
            datasets[split]['loader'] = get_eval_loader(
                loader=config.eval_loader,
                dataset=datasets[split]['dataset'],
                grouper=train_grouper,
                batch_size=config.batch_size,
                **config.loader_kwargs)

    # Set fields
    datasets[split]['split'] = split
    datasets[split]['name'] = full_dataset.split_names[split]
    datasets[split]['verbose'] = verbose

    
    print(transform_name)
    print("initialize model")
    # Initiate model and run on training set
    alg = initialize_algorithm(config, datasets, train_grouper)
    
    # TODO: pytorch load pretrained weights
    
        
    model_file = './best_models/{}_{}_best_model.pth'.format(dataset_name, algorithm_name)
    assert os.path.isfile(model_file), "model weights file not found in ./best_models/ directory"
    
    
    alg.load_state_dict(torch.load(model_file)['algorithm'])
    alg.model.cuda()
    
    print("initialization complete")
    
    
    print("generating predictions")
    it = iter(eval_loader)
    predictions = []
    labels = []
    metadata = []
    
    for batch in tqdm(it):
        
        print("batch:", batch)
        print("batch length:", len(batch))

        for sample in batch[0]: 
            print("batch[0] length:", len(batch[0]))
            print("sample_size:", np.shape(sample))
            
            sample = sample.cuda()
            raw_pred = alg.model(sample)
            raw_pred = raw_pred.cpu().detach().numpy()
            
            print("raw pred length:", len(raw_pred))
            print("pre-predictions:", predictions)
            predictions.extend(raw_pred.tolist())
            print("post-predictions:", predictions)
        
        for i in range(num_samples):
            print("pre-labels:", labels)
            labels.extend(batch[1].tolist())
            print("label:", batch[1])
            print("post-labels:", labels)
            metadata.extend(batch[2].tolist()) # tensor of (64,16) for each batch

        return
    print(len(predictions))
    print(len(labels))
    print(len(metadata))

    
    if num_samples == 1:
        file_name = saved_file_dir + "/" + transform_name + ".npy"
        
    else:
        file_name = saved_file_dir + "/" + transform_name + "_" + str(num_samples) + ".npy"
    
    print(file_name)
    #parameters = locals()                      
    with open(file_name, 'wb+') as file:
        np.save(file, predictions)
        np.save(file, labels)
        np.save(file, metadata)
        #np.save(file, parameters)

    print(file_name)
    eval_data_info = (eval_data.indices, eval_data.dataset)  
    return eval_data_info






def get_eval_dataset(dataset_name, algorithm_name):
    """
    Retrieves a list with content, label, and metadata of all inputs used to form the evaluation data split.
    
    Parameters:
        dataset_name (str): The name of the dataset to be retrieved. 
                            Currently accepts 'civilcomments' and 'amazon'
    Returns:
        eval_inputs:        A list where each index contains a tuple of length three: 
                            text content in the first index (type string),
                            label in the second index (type PyTorch tensor),
                            and metadata in the third index (type PyTorch tensor).

                            The length of eval_inputs yields number of inputs in evaluation data split.

    
    """
    set_seed(0)


    full_dataset = get_dataset(dataset=dataset_name, download=False, root_dir = './wilds/data')
    
    # Grab config generated by WILDS library from txt file
    
    config_file = './model_configs/{}_{}_config.txt'.format(dataset_name, algorithm_name)
        
    assert os.path.isfile(config_file), "model configuration file not found in ./model_configs/ directory"
    
   
    infile = open(config_file,'rb')
    new_dict = vars(pickle.load(infile))
    infile.close() 


    # Create config with ERM algorithm
    config = Namespace(**new_dict)
    
        # Prepare training data, loader, and grouper
    eval_transform = initialize_transform(transform_name=config.transform,
                                          config=config,
                                          dataset=full_dataset,
                                          is_training=False)
    
    eval_data = full_dataset.get_subset('test', transform=eval_transform)
    eval_indices = eval_data.indices.tolist()
    
    eval_inputs = [full_dataset[i] for i in eval_indices]
    return eval_inputs
        

    
    

"""
Calculates accuracy of predictions from a given prediction file.

Parameters:
    prediction_file (str)   : Path to .npy file containing predictions, true labels, and metadata generated using a given                                 augmentation and model algorithm.
    
    sampling_percent (float): Decimal representation of percent of predictions to be included in each subsample when                                     calculating accuracy. Default value is 0.8.
    
    num_runs (int)          : Number of subsample groupings to be created and averaged in calculating accuracy. 
                              Default value is 10. 
                              

Returns:
    accuracy_scores (list)  : List of length num_runs containing accuracy scores (as floats between 0 and 1, inclusive).

"""
def calculate_accuracy(prediction_file, sampling_percent = 0.8, num_runs = 10, accuracy_metric = "exact"):
    set_seed(0)


    logit_predictions = []
    labels = []
    
    with open(prediction_file, 'rb') as file:
        logit_predictions = np.load(file, allow_pickle = True)
        labels = np.load(file, allow_pickle = True)

    accuracy_scores = []
    
    total_dp = len(logit_predictions) #133782
    
    num_subsamples = int(total_dp * sampling_percent) #107025 for 133782 data points

    for i in tqdm(range(num_runs)):
        
        np.random.seed(i)
        random_indices = np.random.choice(np.arange(total_dp), size = num_subsamples, replace = False)
        

        rand_logit_predictions = np.take(logit_predictions, random_indices, axis = 0)
        matching_labels = np.take(labels, random_indices)
        #print("labels_length", len(matching_labels))
        #print("samples_length", rand_logit_predictions[0])
        
        classified_predictions = []
        for prediction in rand_logit_predictions:
            index_prediction = np.argmax(prediction, axis = 0).tolist()
            classified_predictions.append(index_prediction)
    
        if accuracy_metric == "exact":
            score = sklearn.metrics.accuracy_score(matching_labels, classified_predictions)
            
        elif accuracy_metric == "MSE":
            score = sklearn.metrics.mean_absolute_error(matching_labels, classified_predictions)
            
            
        #print(score)
        accuracy_scores.append(score)
    
    return accuracy_scores




"""
Calculates accuracy of predictions from a given prediction file.

Parameters:
    prediction_file (str)   : Path to .npy file containing predictions, true labels, and metadata generated using a given                                 augmentation and model algorithm.
    
    sampling_percent (float): Decimal representation of percent of predictions to be included in each subsample when                                     calculating accuracy. Default value is 0.8.
    
    num_runs (int)          : Number of subsample groupings to be created and averaged in calculating accuracy. 
                              Default value is 10. 
                              

Returns:
    recall_scores (list)  : List of length num_runs containing recall scores (as floats between 0 and 1, inclusive).

"""
def calculate_recall(prediction_file, sampling_percent = 0.8, num_runs = 10):
    set_seed(0)

    logit_predictions = []
    labels = []
    recall_scores = []
    
    with open(prediction_file, 'rb') as file:
        logit_predictions = np.load(file)
        labels = np.load(file)
        
    total_dp = len(logit_predictions) #133782
    num_subsamples = int(total_dp * sampling_percent) #107025 for 133782 data points

    
    
    
    for i in tqdm(range(num_runs)):
        random_indices = np.random.choice(np.arange(total_dp), size = num_subsamples, replace = False)

        rand_logit_predictions = np.take(logit_predictions, random_indices, axis = 0)
        matching_labels = np.take(labels, random_indices)
        

        classified_predictions = []
        for prediction in rand_logit_predictions:
            index_prediction = np.argmax(prediction, axis = 1).tolist()

            classified_predictions.extend(index_prediction)
            
        score = sklearn.metrics.recall_score(matching_labels, classified_predictions)
        recall_scores.append(score)
        
    return recall_scores




def show_values(axs, orient="v", space=.01):
    def _single(ax):
        if orient == "v":
            for p in ax.patches:
                _x = p.get_x() + p.get_width() / 2
                _y = p.get_y() + p.get_height() + (p.get_height()*0.01)
                value = '{:.3f}'.format(p.get_height())
                ax.text(_x, _y, value, ha="center") 
        elif orient == "h":
            for p in ax.patches:
                _x = p.get_x() + p.get_width() + float(space)
                _y = p.get_y() + p.get_height() - (p.get_height()*0.5)
                value = '{:.3f}'.format(p.get_width())
                ax.text(_x, _y, value, ha="left")

    if isinstance(axs, np.ndarray):
        for idx, ax in np.ndenumerate(axs):
            _single(ax)
    else:
        _single(axs)
        
        
        
        

"""
Function that calculates raw number of corrections and corruptions generated by an augmentation.

Parameters:
    
    aug_pred_file (str)                : String containing path to .npy file with logit predictions and labels from                                                  augmentation model.
    
    original_pred_file (str)           : String containing path to .npy file with logit predictions and labels from                                                  original model.
    
Returns:

    corrections_corruptions_dict (dict): Dictionary with two entries: "corruptions" key mapped to number (int) of raw                                                corruptions created by augmentation, and "corrections" key mapped to number (int)                                          of raw corrections created by augmentation.
    

"""
def calculate_corrections_corruptions(aug_pred_file, original_pred_file):
    aug_logit_predictions = []
    aug_classified_predictions = []
    
    original_logit_predictions = []
    original_classified_predictions = []
    
    aug_labels = []
    original_labels = []
    
    with open(aug_pred_file, 'rb') as aug_file:
        aug_logit_predictions = np.load(aug_file, allow_pickle = True)
        aug_labels = np.load(aug_file, allow_pickle = True)
    
    with open(original_pred_file, 'rb') as original_file:
        original_logit_predictions = np.load(original_file, allow_pickle = True)
        original_labels = np.load(original_file, allow_pickle = True)
        
    assert aug_labels.all() == original_labels.all(), "Test set labels do not match between aug. and original."
    assert aug_labels.size == original_labels.size
    
    for prediction in aug_logit_predictions:
        index_prediction = np.argmax(prediction).tolist()
        
        aug_classified_predictions.append(index_prediction)
    
    for prediction in original_logit_predictions:
        index_prediction = np.argmax(prediction).tolist()
        
        original_classified_predictions.append(index_prediction)
    
    
    corrections_count = 0
    corrections_indices = []
    
    corruptions_count = 0
    corruptions_indices = []
    
    for i in range(len(aug_labels)):
        label = original_labels[i]
        original_pred = original_classified_predictions[i]
        aug_pred = aug_classified_predictions[i]
        
        
        # if corruption
        if (original_pred == label and aug_pred != label):
            corruptions_count += 1
            corruptions_indices.append(i)
        
        # if correction
        elif (original_pred != label and aug_pred == label):
            corrections_count += 1
            corrections_indices.append(i)
    
    corrections_corruptions_dict = {"num_corrections": corrections_count
                                    , "num_corruptions": corruptions_count
                                   , "corrections_indices": corrections_indices
                                   , "corruptions_indices": corruptions_indices}
    
    return corrections_corruptions_dict
    
    
    
def extract_corrections_corruptions_inputs(augmentation_file_path, standard_file_path, correction_corruption_type, dataset_name, algorithm_name):
    """
    Retrieves comments, labels, and metadata for either corrections or corruptions made by a provided augmentation.

    Parameters:
        augmentation_file_path (str)    : Full path to augmentation prediction file.
        standard_file_path (str)        : Full path to non-augmented prediction file.
        correction_corruption_type (str): Either "correction" or "corruption" to indicate if correction or corruption data                                           is to be returned.

    Returns:
        data (tuple)                    : A tuple of length 3, with :
                                            - the first index containing a list of the comments (type string) of interest.
                                            - the second index containing a list of labels (type int) for each of the                                                     comments in the first index.
                                            - the third index containing a list of tensors containing the metadata for each                                               of the comments in the first index.
    """
    
    eval_inputs = get_eval_dataset(dataset_name = dataset_name, algorithm_name = algorithm_name)
    print(eval_inputs)
    corrections_corruptions_dict = calculate_corrections_corruptions(augmentation_file_path, standard_file_path)
    
    if correction_corruption_type == 'correction':
        indices = corrections_corruptions_dict['corrections_indices']
        
    elif correction_corruption_type == 'corruption':
        indices = corrections_corruptions_dict['corruptions_indices']
    
    inputs = []
    labels = []
    metadata = []
    
    for i in indices:
        inputs.append(eval_inputs[i][0])
        labels.append(eval_inputs[i][1].item())
        metadata.append(eval_inputs[i][2])
        
    data = (inputs, labels, metadata)
    return data



