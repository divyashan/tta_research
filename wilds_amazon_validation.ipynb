{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67de1438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Using GPU:3\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Set-up: Import numpy and assign GPU\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from gpu_utils import restrict_GPU_pytorch\n",
    "restrict_GPU_pytorch('3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "17b727b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Amazon WILDS pre-trained model\n",
    "import statistics\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import argparse\n",
    "import pdb\n",
    "\n",
    "from wilds import get_dataset\n",
    "from wilds.common.data_loaders import get_train_loader, get_eval_loader\n",
    "from wilds.common.grouper import CombinatorialGrouper\n",
    "from transforms_helenl import initialize_transform, getBertTokenizer\n",
    "\n",
    "sys.path.insert(0, './wilds/examples/')\n",
    "from algorithms.initializer import initialize_algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df53ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec2aa21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "        \n",
    "#config_dict = pickle.load(open('amazon_config.txt', 'rb'))\n",
    "\n",
    "\n",
    "\n",
    "full_dataset = get_dataset(dataset='civilcomments', download=False, root_dir = './wilds/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9955abe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset.split_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b4135c",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Grab config generated by WILDS library from txt file\n",
    "    infile = open('civilcomments_config.txt','rb')\n",
    "    new_dict = vars(pickle.load(infile))\n",
    "    infile.close() \n",
    "\n",
    "    # Create config with ERM algorithm\n",
    "    config = Namespace(**new_dict)\n",
    "\n",
    "    new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ef373b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_augmented_labels(transform_name):\n",
    "    \n",
    "    # Grab config generated by WILDS library from txt file\n",
    "    infile = open('civilcomments_config.txt','rb')\n",
    "    new_dict = vars(pickle.load(infile))\n",
    "    infile.close() \n",
    "\n",
    "    # Modify config with intended transformation\n",
    "    new_dict['transform'] = transform_name\n",
    "\n",
    "    # Create config with ERM algorithm\n",
    "    config = Namespace(**new_dict)\n",
    "    config.algorithm = 'ERM'\n",
    "    \n",
    "    # Generate training and evaluation transforms\n",
    "    train_transform = initialize_transform(transform_name=config.transform,\n",
    "                                      config=config,\n",
    "                                      dataset=full_dataset,\n",
    "                                      is_training=True)\n",
    "    eval_transform = initialize_transform(transform_name=config.transform,\n",
    "                                      config=config,\n",
    "                                      dataset=full_dataset,\n",
    "                                      is_training=False)\n",
    "    \n",
    "    \n",
    "    # Prepare training data, loader, and grouper\n",
    "    train_data = full_dataset.get_subset('train', transform=train_transform)\n",
    "    train_loader = get_train_loader('standard', train_data, batch_size=64)\n",
    "    train_grouper = CombinatorialGrouper(dataset=full_dataset, groupby_fields=config.groupby_fields)\n",
    "    \n",
    "        # Prepare training data, loader, and grouper\n",
    "        \n",
    "    eval_data = full_dataset.get_subset('test', transform=eval_transform)\n",
    "    eval_loader = get_eval_loader('standard', eval_data, batch_size = 64)\n",
    "    \n",
    "    # CODE TAKEN FROM WILDS TRAINING SCRIPTS:\n",
    "    datasets = defaultdict(dict)\n",
    "    for split in full_dataset.split_dict.keys():\n",
    "        if split=='train':\n",
    "            transform = train_transform\n",
    "            verbose = True\n",
    "        elif split == 'val':\n",
    "            transform = eval_transform\n",
    "            verbose = True\n",
    "        else:\n",
    "            transform = eval_transform\n",
    "            verbose = False\n",
    "        # Get subset\n",
    "        datasets[split]['dataset'] = full_dataset.get_subset(\n",
    "            split,\n",
    "            frac=config.frac,\n",
    "            transform=transform)\n",
    "\n",
    "        if split == 'train':\n",
    "            datasets[split]['loader'] = get_train_loader(\n",
    "                loader=config.train_loader,\n",
    "                dataset=datasets[split]['dataset'],\n",
    "                batch_size=config.batch_size,\n",
    "                uniform_over_groups=config.uniform_over_groups,\n",
    "                grouper=train_grouper,\n",
    "                distinct_groups=config.distinct_groups,\n",
    "                n_groups_per_batch=config.n_groups_per_batch,\n",
    "            **config.loader_kwargs)\n",
    "        else:\n",
    "            datasets[split]['loader'] = get_eval_loader(\n",
    "                loader=config.eval_loader,\n",
    "                dataset=datasets[split]['dataset'],\n",
    "                grouper=train_grouper,\n",
    "                batch_size=config.batch_size,\n",
    "                **config.loader_kwargs)\n",
    "\n",
    "    # Set fields\n",
    "    datasets[split]['split'] = split\n",
    "    datasets[split]['name'] = full_dataset.split_names[split]\n",
    "    datasets[split]['verbose'] = verbose\n",
    "\n",
    "    # Loggers\n",
    "    # datasets[split]['eval_logger'] = BatchLogger(\n",
    "    #     os.path.join(config.log_dir, f'{split}_eval.csv'), mode=mode, use_wandb=(config.use_wandb and verbose))\n",
    "    # datasets[split]['algo_logger'] = BatchLogger(\n",
    "    #     os.path.join(config.log_dir, f'{split}_algo.csv'), mode=mode, use_wandb=(config.use_wandb and verbose))\n",
    "\n",
    "    print(transform_name)\n",
    "    print(\"initialize model\")\n",
    "    # Initiate model and run on training set\n",
    "    model = initialize_algorithm(config, datasets, train_grouper)\n",
    "    model.model.cuda()\n",
    "    \n",
    "    print(\"initialization complete\")\n",
    "    \n",
    "    \n",
    "    print(\"generating predictions\")\n",
    "    it = iter(eval_loader)\n",
    "    predictions = []\n",
    "    true_values = []\n",
    "    \n",
    "    for batch in tqdm(it):\n",
    "        \n",
    "        #pdb.set_trace()\n",
    "        raw_pred = model.model(batch[0].cuda()).cpu().detach().numpy().tolist()\n",
    "        #softmax_prediction = sp.special.softmax(raw_pred.cpu().detach().numpy()).tolist()\n",
    "        predictions.append(raw_pred)\n",
    "        true_values.extend(batch[1].tolist())\n",
    "    \n",
    "\n",
    "    print(\"writing predictions\")\n",
    "    file_name = './ERM_predictions/' + transform_name + \".npy\"\n",
    "    \n",
    "    with open(file_name, 'wb+') as file:\n",
    "        np.save(file, predictions)\n",
    "        np.save(file, true_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c03b92fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "supported_augment_labels = ['bert'\n",
    "                           #, 'nlp_ocr'\n",
    "                           #, 'nlp_keyboard'\n",
    "                           , 'nlp_random_char_insert'\n",
    "                           , 'nlp_random_char_substitution'\n",
    "                           , 'nlp_random_char_swap'\n",
    "                           , 'nlp_random_char_deletion'\n",
    "                           , 'nlp_spelling_substitution'\n",
    "                           , 'nlp_random_similar_word_insertion_word2vec_embedding'\n",
    "                           , 'nlp_random_similar_word_insertion_glove_embedding'\n",
    "                           , 'nlp_random_similar_word_insertion_fasttext_embedding'\n",
    "                           , 'nlp_random_similar_word_substitution_word2vec_embedding'\n",
    "                           , 'nlp_random_similar_word_substitution_glove_embedding'\n",
    "                           , 'nlp_random_similar_word_substitution_fasttext_embedding'\n",
    "                           , 'nlp_random_similar_word_substitution_tfidf_embedding'\n",
    "                           , 'nlp_random_contextual_word_insertion_bert_uncased_embedding'\n",
    "                           , 'nlp_random_contextual_word_insertion_bert_cased_embedding'\n",
    "                           , 'nlp_random_contextual_word_insertion_distilbert_uncased_embedding'\n",
    "                           , 'nlp_random_contextual_word_insertion_distilbert_cased_embedding'\n",
    "                           , 'nlp_random_contextual_word_insertion_roberta_base_embedding'\n",
    "                           , 'nlp_random_contextual_word_insertion_distilroberta_base_embedding'\n",
    "                           , 'nlp_random_contextual_word_insertion_xlnet_embedding'\n",
    "                           , 'nlp_random_contextual_word_insertion_bart_base_embedding'\n",
    "                           , 'nlp_random_contextual_word_insertion_squeezebert_uncased_embedding'\n",
    "                           , 'nlp_random_contextual_word_substitution_bert_uncased_embedding'\n",
    "                           , 'nlp_random_contextual_word_substitution_bert_cased_embedding'\n",
    "                           , 'nlp_random_contextual_word_substitution_distilbert_uncased_embedding'\n",
    "                           , 'nlp_random_contextual_word_substitution_distilbert_cased_embedding'\n",
    "                           , 'nlp_random_contextual_word_substitution_roberta_embedding'\n",
    "                           , 'nlp_random_contextual_word_substitution_distilroberta_base_embedding'\n",
    "                           , 'nlp_random_contextual_word_substitution_xlnet_embedding'\n",
    "                           , 'nlp_random_contextual_word_substitution_bart_base_embedding'\n",
    "                           , 'nlp_random_contextual_word_substitution_squeezebert_uncased_embedding'\n",
    "                           , 'nlp_wordnet_synonym'\n",
    "                           , 'nlp_ppdb_synonym'\n",
    "                           , 'nlp_antonym'\n",
    "                           , 'nlp_random_word_swap'\n",
    "                           , 'nlp_random_word_delete'\n",
    "                           , 'nlp_random_crop'\n",
    "                           , 'nlp_random_token_split'\n",
    "                           , 'nlp_back_translation_aug'\n",
    "                           , 'nlp_contextual_sentence_insertion_gpt2_embedding'\n",
    "                           , 'nlp_contextual_sentence_insertion_xlnet_cased_embedding'\n",
    "                           , 'nlp_contextual_sentence_insertion_distilgpt2_embedding'\n",
    "                           , 'nlp_abstractive_summarization_bart_large_cnn'\n",
    "                           , 'nlp_abstractive_summarization_t5_small'\n",
    "                           , 'nlp_abstractive_summarization_t5_base'\n",
    "                           , 'nlp_abstractive_summarization_t5_large']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53aa98b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(prediction_file):\n",
    "    logit_predictions = []\n",
    "    labels = []\n",
    "    \n",
    "    with open(prediction_file, 'rb') as file:\n",
    "        logit_predictions = np.load(file, allow_pickle = True)\n",
    "        labels = np.load(file, allow_pickle = True)\n",
    "    \n",
    "    '''\n",
    "    print(len(logit_predictions[0]))\n",
    "    print(type(logit_predictions))\n",
    "    print(logit_predictions)\n",
    "    '''\n",
    "    \n",
    "    softmaxed_predictions = []\n",
    "    \n",
    "    for pred in logit_predictions[0]:\n",
    "        softmax = sp.special.softmax(pred).tolist()\n",
    "        softmaxed_predictions.append(softmax[1])\n",
    "        \n",
    "    #plt.hist(logit_predictions)\n",
    "    #plt.hist(softmaxed_predictions)\n",
    "    \n",
    "    #return sklearn.metrics.roc_auc_score(labels[:64], softmaxed_predictions)\n",
    "\n",
    "    classified_predictions = []\n",
    "    for prediction in logit_predictions:\n",
    "        index_prediction = np.argmax(prediction, axis = 1).tolist()\n",
    "        \n",
    "        classified_predictions.extend(index_prediction)\n",
    "    \n",
    "    unique, counts = np.unique(np.array(classified_predictions), return_counts = True)\n",
    "    print(unique, counts)\n",
    "    print(len(labels))\n",
    "                                \n",
    "    '''\n",
    "    for sublist in true_values:\n",
    "        labels += sublist\n",
    "    '''\n",
    "        \n",
    "    print(\"PREDICTIONS:\", len(classified_predictions))\n",
    "    #print(\"PREDICTIONS SAMPLE:\", classified_predictions[0])\n",
    "    print(\"TRUE VALUES:\", len(labels))\n",
    "    #print(\"TRUE VALUES SAMPLE:\", len(labels[0]))\n",
    "    #print(type(true_values))\n",
    "    \n",
    "    score = sklearn.metrics.accuracy_score(labels, classified_predictions)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edcc3b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(prediction_file):\n",
    "    logit_predictions = []\n",
    "    \n",
    "    with open(prediction_file, 'rb') as file:\n",
    "        logit_predictions = np.load(file)\n",
    "        true_values = np.load(file)\n",
    "        \n",
    "    print(true_values)\n",
    "    classified_predictions = []\n",
    "    for prediction in logit_predictions:\n",
    "        if prediction[0][0] > prediction[0][1]:\n",
    "            classified_predictions.append(0)\n",
    "            \n",
    "        else:\n",
    "            classified_predictions.append(1)\n",
    "    \n",
    "    score = sklearn.metrics.recall_score(true_values, classified_predictions)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "861e0f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create graph comparing accuracy of all supported augmentations\n",
    "\n",
    "Parameters:\n",
    "    augmentations: list of strings, each corresponding to a supported augmentation\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "def create_accuracy_graph(augmentations):\n",
    "    \n",
    "    aug_to_acc = []\n",
    "    for augmentation in augmentations:\n",
    "        file_name = './ERM_predictions/' + augmentation + \".npy\"\n",
    "        accuracy = calculate_accuracy(file_name)\n",
    "        \n",
    "        aug_dict = {\"augmentation\":augmentation, \"accuracy\": accuracy}\n",
    "        aug_to_acc.append(aug_dict)\n",
    "\n",
    "    \n",
    "    acc_df = pd.DataFrame(aug_to_acc)\n",
    "    \n",
    "    sns.barplot(x = 'augmentation', y = 'accuracy', data = acc_df, ci = None)\n",
    "\n",
    "\"\"\"\n",
    "Create graph comparing accuracy of all supported augmentations\n",
    "\n",
    "Parameters:\n",
    "    augmentations: list of strings, each corresponding to a supported augmentation\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "def create_augmentation_impact_graph(augmentations):\n",
    "    aug_to_mean_acc = []\n",
    "    \n",
    "    original_accuracy = calculate_accuracy('./ERM_predictions/bert.npy')\n",
    "\n",
    "    for augmentation in augmentations:\n",
    "        file_name = './ERM_predictions/' + augmentation + \".npy\"\n",
    "        augmentation_accuracy = calculate_accuracy(file_name)\n",
    "        \n",
    "        print(type(augmentation_accuracy))\n",
    "        print(type(original_accuracy))\n",
    "        mean_accuracy = sum(augmentation_accuracy, original_accuracy) / 2\n",
    "    \n",
    "\n",
    "        augmentation_name = augmentation[4:] + \"+bert\" # remove 'nlp_' prefix in augmentation name\n",
    "        \n",
    "        aug_dict = {\"augmentation\":augmentation_name, \"accuracy\": mean_accuracy}\n",
    "        aug_to_mean_acc.append(aug_dict)\n",
    "    \n",
    "    acc_df = pd.DataFrame.from_dict(aug_to_mean_acc)\n",
    "    \n",
    "    sns.barplot(x = 'augmentation', y = 'accuracy', data = acc_df, ci = None)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3aef8bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': 'civilcomments', 'algorithm': 'ERM', 'root_dir': 'data', 'split_scheme': 'official', 'dataset_kwargs': {}, 'download': False, 'frac': 1.0, 'version': None, 'loader_kwargs': {'num_workers': 1, 'pin_memory': True}, 'train_loader': 'standard', 'uniform_over_groups': False, 'distinct_groups': None, 'n_groups_per_batch': 4, 'batch_size': 16, 'eval_loader': 'standard', 'model': 'distilbert-base-uncased', 'model_kwargs': {}, 'transform': 'bert', 'target_resolution': None, 'resize_scale': None, 'max_token_length': 300, 'loss_function': 'cross_entropy', 'loss_kwargs': {}, 'groupby_fields': ['black', 'y'], 'group_dro_step_size': None, 'coral_penalty_weight': 10.0, 'irm_lambda': 1.0, 'irm_penalty_anneal_iters': None, 'algo_log_metric': 'accuracy', 'val_metric': 'acc_wg', 'val_metric_decreasing': False, 'n_epochs': 5, 'optimizer': 'AdamW', 'lr': 1e-05, 'weight_decay': 0.01, 'max_grad_norm': 1.0, 'optimizer_kwargs': {}, 'scheduler': 'linear_schedule_with_warmup', 'scheduler_kwargs': {'num_warmup_steps': 0}, 'scheduler_metric_split': 'val', 'scheduler_metric_name': None, 'process_outputs_function': 'multiclass_logits_to_pred', 'evaluate_all_splits': True, 'eval_splits': [], 'eval_only': False, 'eval_epoch': None, 'device': 0, 'seed': 0, 'log_dir': './logs', 'log_every': 50, 'save_step': None, 'save_best': True, 'save_last': True, 'save_pred': True, 'no_group_logging': False, 'use_wandb': False, 'progress_bar': False, 'resume': False}\n"
     ]
    }
   ],
   "source": [
    "predict_augmented_labels('bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "03be07fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert\n",
      "initialize model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertClassifier: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertClassifier were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialization complete\n",
      "generating predictions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a533f6f9975c4e588f45fd4775377aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-ee65c496b71b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0maugment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msupported_augment_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpredict_augmented_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-680eeee748ad>\u001b[0m in \u001b[0;36mpredict_augmented_labels\u001b[0;34m(transform_name)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;31m#pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mraw_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;31m#softmax_prediction = sp.special.softmax(raw_pred.cpu().detach().numpy()).tolist()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for augment in supported_augment_labels:\n",
    "    predict_augmented_labels(augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8cbe0e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3795454545454546"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "calculate_accuracy('./ERM_predictions/nlp_ocr.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "61893260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5375"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_accuracy('./ERM_predictions/bert.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a19feab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133782\n",
      "133782\n",
      "PREDICTIONS: 133782\n",
      "TRUE VALUES: 133782\n",
      "133782\n",
      "133782\n",
      "PREDICTIONS: 133782\n",
      "TRUE VALUES: 133782\n",
      "133782\n",
      "133782\n",
      "PREDICTIONS: 133782\n",
      "TRUE VALUES: 133782\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_accuracy_graph(supported_augment_labels)\n",
    "#create_accuracy_graph(['bert'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "305702d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 ... 0 0 0]\n",
      "PREDICTIONS: 133782\n",
      "TRUE VALUES: 133782\n",
      "[1 0 0 ... 0 0 0]\n",
      "PREDICTIONS: 133782\n",
      "TRUE VALUES: 133782\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-3db40791629e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcreate_augmentation_impact_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupported_augment_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-63-0f1bd10e40e3>\u001b[0m in \u001b[0;36mcreate_augmentation_impact_graph\u001b[0;34m(augmentations)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maugmentation_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mmean_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maugmentation_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_accuracy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not iterable"
     ]
    }
   ],
   "source": [
    "create_augmentation_impact_graph(supported_augment_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "27863c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.45754883 0.54245126]]\n"
     ]
    }
   ],
   "source": [
    "with open('nlp_ocr_ERM_predictions.npy', 'rb') as g:#, open('nlp_random_char_insert_ERM_predictions.npy', 'rb') as h:\n",
    "    a = np.load(g)\n",
    "    #b = np.load(g)\n",
    "    #c = np.load(h)\n",
    "    \n",
    "print(a)#, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec103fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a553c59c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch[1])\n",
    "batch[1]\n",
    "#batch[2]\n",
    "\n",
    "# batch[0] is array of input features\n",
    "# batch[1] is array of labels\n",
    "# batch[2] is array of metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d9576c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 2])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(batch[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6e2aefa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 300, 2])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(batch[0][0])\n",
    "\n",
    "np.shape(torch.unsqueeze(batch[0][0], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef7eee3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model.model(torch.unsqueeze(batch[0][0], 0).cuda())\n",
    "\n",
    "# Outputting before transforming to measure toxicity\n",
    "\n",
    "# Apply softmax to normalize and sum to 1\n",
    "\n",
    "# Save predictions while iterating through samples into .npy or pickl (np.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53d72faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.4568701684474945, 0.5431299209594727]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_prediction = sp.special.softmax(x.cpu().detach().numpy())\n",
    "softmax_prediction.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebc916b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': <wilds.datasets.wilds_dataset.WILDSSubset at 0x7fa531852a58>,\n",
       " 'loader': <torch.utils.data.dataloader.DataLoader at 0x7fa531852a90>,\n",
       " 'split': 'test',\n",
       " 'name': 'Test',\n",
       " 'verbose': False}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "49f5d812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fa5318529b0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['val']['loader']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
